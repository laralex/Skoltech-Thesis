MASTER’S THESIS 
Real-Time Neural Avatar Rendering on Mobile Devices Master’s Educational Program: Information Science and Technology 
Student 
Alexey S. Larionov 
Information Science and Technology 
June 3, 2022 
Research Advisor: 
Victor S. Lempitsky 
Associate Professor 
Co-Advisor: 
Renat M. Bashirov 
Leading Research Engineer 
Moscow 2022 
All rights reserved.© 
The author hereby grants to Skoltech permission to reproduce and to distribute publicly paper and electronic copies of this thesis document in whole and in part in any medium now known or hereafter created.
МАГИСТЕРСКАЯ ДИССЕРТАЦИЯ 
Нейронный рендеринг аватаров в реальном времени на мобильных устройствах 
Магистерская образовательная программа: Информационные науки и технологии 
Студент 
Алексей С. Ларионов 
Информационные науки и технологии 
3 Июня, 2022 
Научный руководитель: 
Виктор С. Лемпицкий 
доцент 
Со-руководитель: 
Ренат М. Баширов 
ведущий инженер-исследователь 
Москва 2022 
Все права защищены.© 
Автор настоящим дает Сколковскому институту науки и технологий разрешение на воспроизводство и свободное распространение бумажных и электронных копий настоящей диссертации в целом или частично на любом ныне существующем или созданном в будущем носителе.
Real-Time Neural Avatar Rendering on Mobile Devices 
Alexey S. Larionov 
Submitted to the Skolkovo Institute of Science and Technology 
on June 3, 2022 
Abstract 
During development of artificial intelligence models, the utmost attention is brought to qual ity (accuracy) of their outputs. However, some models are meant to be executed on limited user hardware. Thus, computational resources also require careful consideration, when it is necessary to sustain high performance. 
This master thesis describes implementation of an application with an integrated Deep Neural Network, for mobile devices on Qualcomm Snapdragon platform. The neural network generates images of full-body 3D human avatars. The generation takes into account position and orientation of the mobile device. When avatar images are visualized on top of camera frames, the Augmented Reality (AR) effect emerges. A user can observe the avatar from different angles, either from afar or close up. Thus, the generated images should keep the visual quality from angles, unseen in the training data. The first contribution of this thesis is organization of a computational pipeline to achieve real-time performance. Secondly, we carry out research on ways of increasing the visual quality, that would be robust to arbitrary view points. 
As a result, we execute the baseline model [36] in the mobile application in real time, with more than 30 frames per second and up to 640 × 640 pixels resolution. We generate the model’s input, such that the observed portion of the body occupies as much frame space as possible, thus increasing visual quality in the AR application. We extend the training procedure of the neural net work training, to learn and produce images in either full-body scale or close-up scale. We propose solutions to partially prevent visual artifacts that appear with the baseline training pipeline. 
Keywords: Real-Time Performance, Mobile Hardware, Neural Rendering, Deep Neural Network, Human Avatar Reconstruction, Computer Graphics. 
Research Advisor: 
Name: Victor S. Lempitsky 
Degree: PhD 
Title: Associate Professor 
Co-Advisor: 
Name: Renat M. Bashirov 
Degree: MSc 
Title: Leading Research Engineer 
3
Нейронный рендеринг аватаров в реальном времени на мобильных устройствах 
Алексей С. Ларионов 
Представлено в Сколковский институт науки и технологий 
3 Июня, 2022 
Реферат 
При разработке моделей искусственного интеллекта, наибольшее внимание уделяется качеству (точности) результатов. Однако, некоторые модели предполагается выполнять на ограниченном пользовательском аппаратном обеспечении. Для поддержания их высокой производительности, необходимо учитывать затрачиваемые вычислительные ресурсы. 
В данной магистерской диссертации описывается реализация прикладного приложения со встроенной глубинной нейронной сетью, для мобильных устройств на платформе Qualcomm Snapdragon. Приложение визуализирует изображения 3D аватара человека, генерируемые нейронной сетью. Генерация изображений происходит в реальном времени, более 30 кадров в секунду. Изображения аватаров учитывают пространственное положение устройства, и при визуализации поверх изображений с камеры, образуется эффект дополненной реальности (Augmented Reality, AR). Пользователь может осматривать аватар с разных углов, в отдалении и приближении. Следовательно, изображения аватаров должны сохранять качество с углов обзора, отсутствующих в тренировочных данных. Основной вклад данной работы заключается в организации вычислений для достижения производительности в реальном времени. Дополнительно, были исследованы пути повышения визуального качества изображений в приложении с произвольных точек обзора. 
В результате, в мобильном приложении baseline модель [36] может вычисляться в реальном времени, вплоть до разрешения 640 × 640 пикселей. Для повышения визуального качества аватаров, нейронная сеть получает и выдает данные так, чтобы видимая часть аватара занимала как можно больше места в кадре. Исходный подход обучения нейронной сети был дополнен для предотвращения части визуальных артифактов, и для генерации изображений человека как в полный рост, так и в приближении. 
Ключевые слова: Вычисления в реальном времени, Мобильное аппаратное обеспечение, Нейронный рендеринг, Глубинная нейронная сеть, Реконструкция аватаров людей, Компьютерная графика. 
Научный руководитель: 
Имя: Виктор С. Лемпицкий 
Ученое звание, степень: к.ф.-м.н. 
Должность: доцент 
Со-руководитель: 
Имя: Ренат М. Баширов 
Ученое звание, степень: маг-р 
Должность: ведущий инженер-исследователь 
3
Contents 
1 Introduction 6 1.1 Computer Graphics state and issues . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.2 Neural Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.3 Project tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 
2 Literature Review 12 2.1 Traditional 3D representations and rendering algorithms . . . . . . . . . . . . . . 12 2.2 Neural Rendering and human avatars . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.3 Metrics of image similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.4 Ways of speeding up DNN inference . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.5 Computations on mobile hardware . . . . . . . . . . . . . . . . . . . . . . . . . . 24 
3 Methodology 26 3.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.2 Mobile application development . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.3 Improving quality of synthesized images . . . . . . . . . . . . . . . . . . . . . . . 40 
4 Results and Discussion 54 4.1 Mobile application results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4.2 Training experiments results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 4.3 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 
5 Conclusions 62 Acknowledgements 64 
4
Author Contribution 64 Abbreviations 65 A Relevant mobile application code 66 
B Images of the experiments 81 B.1 Auxiliary illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 B.2 Mobile application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 B.3 Experiments on DNN training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 
5
Chapter 1 
Introduction 
1.1 Computer Graphics state and issues 
Computer Graphics is a vast discipline of Computer Sciences. In general, it studies theoreti cal methods for synthesis of 2-dimensional (2D) images using computers. It also includes research on computer hardware, used for image computing and displaying. Among applications of com puter graphics there are: typography, cinema, entertainment, scientific visualization and industrial design. From a physical point of view, the 2D images capture information about how light beams pass through the space and reach our eyes. Then a snapshot of all received light is interpreted by our perception as a sight full of shapes and colors. 
Images in a computer are usually represented with two major archetypes. The first one – vec tor images [28], that are represented by a set of 2D points, lines, polygonal shapes and mathemati cally defined distribution of color within them. They are more suitable for representing schemes or text, while real world objects would require to define hundreds of thousands of shapes to model all unique uneven patterns of materials and light. Such work is enormous in terms of computations, human hand-craft, and computer file sizes. On the other hand, raster images [31] are used. They are represented by a uniform rectangular grid (referred to as raster), filled with colored squares (referred to as pixels). Typically, colors are defined using Red-Green-Blue (RGB) color model [40], where all possible colors are mixed from 3 primary colors, stored in computers as values in [0; 1] range of real numbers (or similarly as values in [0; 255] range of integer numbers). Although the number of pixels may also surpass millions, the structure of pixel grids is uniform, and each pixel stores the same amount of data, allowing fast encoding and decoding in a computer. Also for most cases, the image data has information redundancy in it, and thus can be compressed [46]. Besides, the pixel grid resembles a canvas from the traditional art. The current computer software for creation of raster images has capabilities similar to real brushes and palettes. This intuitive representation allows to speed up production time for humans. It is known that raster images can be very realistic, proven by the visual quality that can be captured with modern digital cameras [7]. 
6
Speaking of realism, it is one of the most important challenges in the computer graphics research. The generated images should ideally be indistinguishable from sights we perceive with our own eyes. Unfortunately, the real world is at least 3-dimensional (3D). Thus, it cannot be fully represented with images on a screen or paper, which are 2D by nature. Another obstacle, is that human vision is very sensitive, and it can easily detect unrealistic or anomalous features, e.g. noise, tearing, blurriness and so on. Those are referred to as visual artifacts (also glitches, distortions). However, to this day there have not been discovered a unified metric of realism, that would allow to compare two similar images and tell which will be more plausible for the human perception [98]. 
Direct usage of 2D images for visualization of the 3D world is not always efficient. For example, if we intend to create an animation (a temporarily coherent sequence of images), it would require to draw the same content with slight adjustment between images. Instead of manually copying the content from one image to another and insuring its coherency and synchronization, it is common to instead approximately model the 3D scene of interest. Such scene representations can take explicit forms, e.g. point clouds, voxel grids, triangular meshes, or even implicit functions, such as signed distance fields (SDF)[92]1. After modeling, at every moment of time the geometry’s appearance can be projected from a certain view point onto an image. This allows to model the scene one time and reuse it for synthesis of dozens of images. 
Typically 2D images are synthesized from a 3D scene, using rendering algorithms. They emulate the laws of physics that lie within synthesis of images with physical cameras. One such algorithm is tracing of rays that travel starting from a light source, bounce from reflective sur faces, until finally they reach an eye or a camera. This method is one of the most photo-realistic, at least to the state of our theoretical understanding of physical specular phenomena. However, it is exceptionally long to compute in great details. That is why a much more coarse algorithm of rasterization is typically used, with each simple geometric part of the scene (e.g. points, triangles, voxels) projected onto a 2D image. The color information for each part is extracted from texture images, that capture physical properties of surfaces, such as color, refraction, reflection, etc (see Figure 1.1). But with either rendering algorithm, there persists a problem that all those proper ties need to be explicitly specified, often by a human, which drastically increases the difficulty of achieving photo-realism. Besides, some 3D scene representations lack details, some are heavy to be efficiently processed, or require enormous amount of computer storage. In either case, an order of magnitude higher number of geometric details needs to be defined compared to just a 2D image. 
1See Section 3.1. of [92]. 
7
  

Figure 1.1: Example of a triangular mesh, with a texture image wrapped on it. Each triangle is mapped to texture coordinates, that in turn allow to sample a color patch on the texture. A rasteriazation process projects every triangle onto an image, and interpolates texture coordinates for every image pixel, to then sample corresponding color values from the texture. Picture source: metalbyexample.com/textures-and-samplers. 
1.2 Neural Rendering 
In the last decades, the methods of Artificial Intelligence (AI) and its sub-fields of Machine Learning (ML) and Deep Neural Networks (DNN) have emerged to solve multidisciplinary prob lems of science and business. Instead of modeling a phenomenon algorithmically, they aim to approximate statistical properties of data captured from the phenomenon. Generally, input data is passed down a highly-parameterized computational graph, and output is compared to certain de sired data. During a process referred to as a optimization (training, tuning), the parameters of the graph are iteratively adjusted to better fit the desired data. Afterwards, the computational graph is expected to be general enough to also accurately model the data, that was never seen during train ing. The automation is what makes these approaches groundbreaking. Unlike humans, numerical algorithms can capture very complex statistical correlations or patterns in highly-dimensional data, including images. 
A selection of computer graphics problems may also be solved by means of ML and DNN methods. A few notable research topics in this field include: 
• image impainting – completion of missing parts on an image with semantically fitting content; • image super-resolution – up-scaling an image and making it more detailed; • reconstruction of 3D objects – automated creation of an object’s 3D representation (see page 7) and materials from 2D images captured with physical cameras; 
• neural rendering – automated synthesis of realistic images with certain expected content. 8
This thesis focuses on the topic of neural rendering, although its definition is quite vague. A default approach is to use traditional algorithms to generate a simple image that describes coarse information about a scene, such as general outlines of each separate object, distance from a camera, etc. Then an AI model is used to generate the final image – realistic, detailed, possibly with higher resolution [93]. There is another branch of research, where the AI is used to represent the 3D scene, often implicitly as weights of the AI model. Then this representation is decoded (sampled) into 2D images using deterministic algorithms [66]. 
It is very rare to have a detailed, realistic and diverse dataset of 3D scenes that we want to reconstruct with neural rendering. Instead, AI models are typically trained on physical camera images, thus combining tasks of 3D object reconstruction and neural rendering. It is known to be a very tough challenge for man-made algorithms, and not as much easier for AI models. On the other hand, there already exist solutions that may surpass any manual human labor on image synthesis or scene modeling [50, 51, 49, 66]. 
However, training an AI model for generating photo-realistic images is only a half of the challenge. Every AI model is meant to be integrated into a real application and executed (inferred) on some computing device. It can be a stationary computer with lots of computing power. It can also be mobile hardware, since in the last years their computing power became powerful enough to infer AI models in real-time. Such hardware is included in modern smartphones, Augmented Reality (AR), or Virtual Reality (VR) head mounted displays. Running DNN models here directly provides lower latency of computations, better privacy and data security. But also, many perfor mance, compute and power usage constraints may arise: 
• AI models that run in real-time applications need to deliver results multiple times per second. • The computing resources are limited due to a small physical size of the hardware. Resources are even further restricted, in order to reduce overheat and to improve battery life. • AI models that utilize customer devices by any means need to deal with compatibility issues (e.g. between operating systems, computing architectures, memory layouts). 
This makes real usage of AI models a very challenging task. AI projects have to rely on stan dardized software and hardware solutions, that allow easier integration of new AI advancements. However, a unified way to efficiently run arbitrary AI models on every hardware is absent. It is often, that instead a particular AI architecture is manually integrated for a particular type of hard ware, squeezing the maximum performance out of them. This is the main way-out for the current projects to obtain any business value from combining state-of-the-art hardware and AI. 
9
1.3 Project tasks 
Samsung AI Center at Moscow had been researching on real-time neural rendering of human avatar images. Their recent DNN architecture [36] takes as an input a rasterized image with a body mesh with absent clothing, hair and distinctive person features. When trained with high image resolution, this architecture has proven to fill in those details on the image, producing good full body (FB) images with a single person. However, the performance concerns for real time avatar rendering were not profoundly researched. 
During the Skoltech’s Summer Immersion of Master students in 2021, a development of a mobile Android application was started from scratch. It needs to serve as a test of performance, visual quality and compatibility of the aforementioned architecture on mobile devices. The appli cation should prepare input data for the DNN and visualize avatar images on top of camera frames as AR, i.e. as if images are part of the captured environment. Camera movements allow to observe the avatar from arbitrary distance and angle. 
The main concern is to obtain a balance of performance and visual quality of the mobile AR application. For instance, with the minimal acceptable avatar resolution of 512 × 512 pix els, the performance of the researched DNNs is too low. Each frame computing takes more than 60 milliseconds (ms) on the most recent modern mobile hardware (Qualcomm Snapdragon 888). However, a computation time lower than 33ms per frame is required for smooth AR experience, which is equivalent to more than 30 frames per second (FPS). Moreover, the architecture’s training process could be adjusted with AR usage in mind. For example, when we move camera close to the avatar, effectively only a small part of the body can be seen by the user. But if the DNN generates full body images regardless, a big part of computations is wasted. And the seen part will be in a fairly low resolution. When rendered on a mobile screen with high resolution, it appears as a pixelated image in low details (see Figure 1.2b). Also the DNN should be stable in views that are possible in AR, but not present in the training data, e.g. straight top-down or bottom-up views. 
The goal of this thesis is to continue the aforementioned project. The first high-level task is to achieve an admissible trade off of performance and visual quality, when executing the DNN as a part of mobile AR experience. Target devices are the latest mobile phones running on Qualcomm Snapdragon System on the Chip (SoC), with Android operating system. The second task is to research on adjusting DNN training to produce better quality images on zoomed scales (upper body, close-up), while retaining similar quality in full-body scale. However, it is advisable to keep the same baseline [36] DNN architecture if possible, because integration of a completely new pipeline may contradict the implementation of the first task, and will require significant time on obtaining better results and integration. 
10
  
  
(a) (b) 
Figure 1.2: (a) A full-body image generated with the baseline DNN [36]. (b) Full body image synthesis for AR, but only a small image part is visible and in low resolution, the rest is wasted. 
The specific tasks are defined in details as follows: 
1. To implement real-time input generation on mobile, for the baseline DNNs. This includes inference of a posed mesh of a unified human body model [74], extraction of camera tracking information, mesh rasterization as a 2D image, its passing to the DNN running on mobile. 
2. To research on decreasing inference time of the baseline [36] DNN architecture on mobile. It should be possible to infer it with real-time performance of at least 30 FPS, and resolution of the generated images above 256 × 256 pixels. 
3. To research on DNN’s training improvements, to make it robust to out-of-distribution views. For example, extremely far zooms, extremely close zooms, rotations and angles of view, while preserving full-body quality. 
4. To achieve execution of the DNNs on specialized mobile hardware for fast quantized AI computations (see Section 2.5). To compare visual difference between outputs of the default model and outputs on mobile. 
5. To research on algorithmic improvements that would compensate weaknesses of plain DNN’s inference, improving either performance or visual quality of avatars. 
11
Chapter 2 
Literature Review 
This chapter introduces to traditional and Deep Learning approaches of image synthesis, and overviews ways of inferring DNNs in real-time on mobile devices. Sections 2.1 and 2.2 gradually expand on state-of-the-art reconstruction of human images, with more attention to the parts that comprise the basis for this work. The used metrics and optimization losses are described in Sec tion 2.3. Then, methods for decreasing DNNs inference time are given (Section 2.4) with a few peculiarities of the mobile devices targeted by this work (Section 2.5). 
2.1 Traditional 3D representations and rendering algorithms 
Ever since the dawn of Computer Graphics, many primitive representations for 3D world were proposed to reconstruct real objects. The representations may model volumetric properties, such as occupancy, density of matter, radiance, color at a certain point of 3D space; or model surfaces and assign these properties to points on a surface. The definition of the primitives may be explicit or implicit. An explicit surface can be defined by taking every point on a real coordinate plane R2, and mapping it to the third ”height” coordinate explicitly via function fe(.) ∈ R. An implicit surface is defined by a zero-level set of a multivariable function fi(.) ∈ R [92]. 
Sexplicit = 
( x 
y 
fe(x, y) 
!       xy ∈ R2) (2.1) 
Simplicit = 
( x y 
z 
! 
∈ R3 
     fi(x, y, z) = 0) (2.2) 
Unlike surfaces, values of volumetric properties are usually defined only explicitly for all points in the 3D space: 
V = 
( 
fvol(x, y, z) 
      xyz!∈ R3)(2.3) 
Let us look at the common surface and volume representations in details. A point cloud is a discrete set of points in Rn(usually R3). It can model either surfaces (thus each point belongs to 
12
the surface) or volumes. The points may be associated with additional attributes, such as: color, density, orientation vector (normal) of a surface. When rendering an image from a point cloud, commonly the points are interpreted as spheres with a certain radius, in order to compensate for sparsity and lack of connectivity between adjacent points [58]. 
A polygonal mesh is the most wide-spread 3D representation, being a compromise between granularity of 3D modeling and computational requirements to render adequate images. It recon structs curvature of a surface using small planar segments, usually triangles or quads, and generally N-sided polygons. A set of 3D points (vertices) is defined, along with enumeration of connected vertices. Although each vertex can be associated with additional attributes, using textures much finer details can be modeled. Usually, textures are 2D images, that store in the pixels values of attributes (color, surface orientation, etc). Each 3D vertex on a mesh has designated 2D texture coordinates (also referred to as UV-coordinates; See Figure 1.1). A barycentric interpolation of vertices’ texture coordinates is used to sample an attribute value from the texture for a certain point on a polygon. Textures allow to model details, that are too hard to approximate directly with poly gons, e.g. small wrinkles, folds of clothes, complex color patterns, etc. Using textures, a variety of information can be modeled, e.g. a normal direction to an abstract fine-detailed surface for all points on polygons, allowing to calculate realistically looking interaction of light with materials, without additional computational overhead [20]. 
A voxel grid is commonly referred to as a 3D image, for also being a regular grid of prim itives - cubes (voxels) with associated attributes. As volumetric representations, they may store occupation or density of matter at all points in a closed 3D area [35]. Voxel grids are fast to pro cess, but are enormously large in memory. Storing fine-detailed geometry requires shrinking the grid’s scale, but memory consumption grows cubically. Thus, attempts on sparse voxel grids were made, using hyerarchical structures, that are longer to query, but can encapsulate large empty space into very large voxels, and reconstruct fine details using very small voxels where needed [15]. 
Implicit functions, as already described, can model surfaces. For example, we can define a Signed Distance Function for all points in a 3D volume. The values tell the closest distance to the surface, with negative sign if the point is ”inside” a surface. However, to render an image, the sur face (i.e. the zero-distance set) has to be queried repeatedly from the implicit representation. This makes image synthesis computationally expensive, unless an intermediate explicit representation is built to approximate the surface, e.g. using Marching Cubes algorithm [63] to reconstruct a mesh. 
The computer graphics evolution led to computing hardware to support some 3D represen tations. For instance, images are now typically rendered using Graphic Processing Units (GPUs), that unlike general Central Processing Units (CPUs) implement operations of linear algebra in a massively parallel way, allowing to process millions of primitives in real-time. Next, we will de 
13
scribe the most common rendering algorithms called ray casting and rasterization. They can be adapted to any 3D representation, although sometimes with too high computational complexity. 
For both algorithms, a virtual camera is mathematically modeled. In physical cameras, light that bounces in the 3D space forms an image as it hits the camera’s sensor. Light can be captured from a larger span of view using lenses, that focus light rays into a single spot, referred to as camera origin or focal point. For virtual cameras, a pinhole model is used, where a focal point is chosen, and the 3D scene representation is projected on a pixel grid, as if it was placed at a distance from the focal point, called focal length. From that, a view frustum is defined, that is a pyramid that extends from the focal point through the image to infinity, and has apex cut off at the focal length (see Figure 3.1). Objects outside the frustum are invisible to the virtual camera. 
The image formation can be mathematically expressed using a projecting transformation K (also called intrinsic matrix, see Formula 2.4), parameterized by focal lengths along axes (fx, fy), image center point (cx, cy), and axis skew γ. This transform maps a 3D point, which is extended to 4D homogeneous space p = [x, y, z, 1] to a 3D point pproj = K · p = [xp, yp, wp]. Dividing by wp, we get pimg = [xp/wp, yp/wp, 1]. The last coordinate can be discarded being equal to 1. Now pimg contains pixel coordinates of the projected 3D point. This transform only simulates a camera at the coordinates origin. To place it at any point in 3D space, an additional affine transformation matrix E (extrinsic, see Formula 2.5) is introduced, which defines camera rotation R and translation t in the homogeneous space [92]. The projection is thus pproj = K · E · p . 
K = 
 
fx γ cx 0 
0 fy cy 0 0 0 1 0 
 
(2.4) E = 
 
R3×3 t3×1 01×3 1 
 
 (2.5) 
Ray casting method uses the above math to project rays, that pass through all pixels of the image and converge at the focal point. At a point where a surface is hit by the ray, surface attributes are extracted from the 3D representation and the pixel is colored accordingly. To model more real istic lighting, the reflection from the hit-points can be calculated, thus attributes from a secondary hit will be accumulated. This recursive process is also referred to as ray tracing. Such approach is computationally heavy, as 3D representation has to be queried many times for each ray to detect hits. On the bright side, the computational complexity only scales with the number of image pixels, no matter the number of objects in the scene (see Figure 2.1a). 
Rasterization method is typically used together with mesh 3D representation [31]. It is much more widely used due to its computational efficiency and hardware support. Here, vertices of all polygons are projected onto the image. Pixels that belong to the projected polygons are colored. Additionally for each pixel, the nearest polygon’s depth is stored as depth image (also called Z 
14
buffer). This helps to resolve occlusion between polygons. Since it is usual that the number of vertices in the view frustum is smaller than the number of pixels, the computation is much faster than ray casting. However, it scales with the number of polygons, requiring to apply algorithms for efficient discarding of occluded polygons in real-time. Also, rasterization loses in producing realistic specular phenomena, such as shadows, since the image formation is not physically-based. 
  
  
(a) (b) 
Figure 2.1: (a) Rays are cast from the camera origin through each pixel, and if a ray hits any surface, the pixel is colored accordingly. (b) In rasterization, vertices of a polygon are projected onto a raster grid, filling the pixels that belong to the projected shape. Images from www.scratchapixel.com. 
2.2 Neural Rendering and human avatars 
As was mentioned in the Section 1.2, any theory that lies inside algorithmic approaches may miss complex correlations in the real world behavior. This can be a corner stone that differentiates realistically-looking and truly photo-realistic images. For this reason, statistical methods of AI (such as DNNs) are applied, to learn a parameterized model from the real observations. In our case, it learns on real photos to synthesize images automatically from descriptive data, such as position of the view in the world, or the desired content in the view. The learning per se comes down to iterative processing of the DNN from input to output, computing loss (fitness) between the output and desired data. The DNN’s weights are updated (optimized) using a gradient value of the loss with respect to every weight. 
There are two major modes of DNNs training. The first it to learn on diverse data to under stand different modalities of it. With this knowledge, a feed-forward with novel inputs would yield an acceptable result [33, 73, 50]. Although time of inference is much smaller than of training, it is still rarely real-time, because such generalization requires a substantial number of learnable pa rameters. Another approach is optimization-based. A DNN is trained only on a narrow domain of the data, requiring fewer parameters to fit. Every novel domain needs training of a separate model. 
15
Next, we will review relevant advances of DNN research for image synthesis. Multi-Layer Perceptron (MLP) is a fully-connected neural network, where layers compute weighted sums of a previous layer, and transform sums by a non-linear function [70]. MLP is known to be a univer sal function approximator [42], yet high accuracy requires lots of parameters. Still, the research community has found it powerful to use MLPs as an implicit 3D scene representation, and then synthesizing the images using deterministic algorithms, such as ray casting [86]. This idea evolved into neural radience fields (NeRF, [66]), where MLP maps any scene point (x, y, z) and view an gle (θ, ϕ) to its color (r, g, b) and volume density σ. The method can model complex materials and reflections from novel views of static scenes, but takes seconds to query the volume and render an image. It is also suitable for reconstruction of dynamic objects, such as humans[14]. 
Large MLPs are prone to overfitting, i.e. that they can memorize perfect solutions to the training data and fail to generalize the novel data. Convolutional Neural Networks (CNN) rev olutionized [59, 56] image processing with fewer parameters and better generalization. There, a series of 2D convolution operations is applied, that compute weighted sums over input tensors with a small-sized learnable matrix in a sliding-window manner. In MLPs the same data in differ ent places on an image is processed with different weights. CNNs are invariant to spatial shifts, but may have small receptive field - i.e. size of recognized patterns limited by kernel size. Many notable DNNs apply convolutional layers, e.g. AlexNet[56] ResNet[37], VGG[85], U-Net[79]. 
Generative Adversarial Networks (GAN, [33]) is an optimization framework for DNNs that advanced image synthesis a lot since CNNs. In essence, it represents a competing game. A gen erator network G learns the data distribution. A discriminator network D learns to classify input as either real observation, or produced by G (fake). Importantly, the losses for G do not need to measure similarity of the real and generated data, it is enough for G to minimize accuracy of D, by making more realistic fakes. Ever since, many GAN models were proposed [25], the most notable of which is StyleGAN [50, 51, 49] that is able to generate artificial high-resolution images of hu man faces, but far from real-time inference and rich articulation. Apart from that, GANs can be a part of architecture to generate any data at training time and optimize with respect to it [36, 11]. 
Speaking of DNN optimization, it is desirable to prevent overfitting and at the same time speed up convergence of results to the optimum (escaping underfitting). Overfitting can be pre vented with more diverse real data, or by making the existing data diverse via augmenting, e.g. replicating data with varying color, erased content, affine transformations. Also, regularization of weights can be applied to make training harder, e.g. dropout[88] – random nullifying of activa tion values; or weight decay[64] – small decrease of weights towards 0 absolute value on every optimization step. 
16
The reason of underfitting can be a deficit of number of parameters to fit the data. Also, a covariate shift can be frequently observed, e.g. training and novel (test) data may have slightly different distributions, or even activations of internal layers may have different distributions after updating weights of a network. To compensate the shift, normalization techniques are applied. Batch Normalization (BN, [47]) algorithm during training normalizes intermediate activations to the mean and variance of the whole batch of data, see Formula 2.6. Here γ and β are learned vectors of same size as number of input channels. Besides, the batch statistics are accumulated on each step with exponential averaging, so that during inference batches of any size could be processed. Instance Normalization (IN, [96]) follows the same formula, but the statistics are computed over each sample, rather than a batch of samples. These and other normalization schemes may increase 
DNNs convergence speed and reduce sensitivity to hyperparameters. 
y =x − E[x] 
pVar[x] + ϵ∗ γ + β (2.6) 
There is a branch of research called deferred neural rendering, where 3D meshes of objects are rendered with a neural texture – a tensor, that unlike usual textures with 3 color channels (RGB), has generally C channels [93]. A DNN is trained to map this neural image to a colorful RGB image with segmentation (RGBS/RGBA), see Figure 2.2b. The motivation is to give a clue to the DNN on what to synthesize, and to optimize the neural texture to implicitly model specular properties of real materials. Also a coarse mesh of the object can be used, that does not model small details, and the DNN will have to put information into the neural texture about reconstruction of these details [36, 78]. Crucially, the algorithm that renders the mesh with the neural texture has to be differentiable [52], to optimize the texture. 
This thesis focuses on human avatars neural rendering, i.e. recovering of photo-realistic ap pearance, and modeling movements of limbs, face, hair, clothes. The reconstruction may cover the full-body (FB) [104, 36, 78], upper-body (UB) [24], or only face of a person [61, 11]. The source data may vary, e.g. a single frontal photo (one-shot avatar), multiple photos from sides (few-shot avatar), a video sequence (video-based avatar). Either can be captured on a monocular camera[36], stereo camera[102], multiple cameras[61, 84]; with or without camera calibration parameters (in trinsics as in Formula 2.4). Images alone impose depth ambiguity, i.e. the true size of objects on an image is unknown. Depending on proximity to the camera, they appear smaller or bigger. It can be resolved with depth sensor information [97], or by scanning 3D meshes of objects [14], both requiring expensive equipment. Thus, it is desired to make realistic avatars from common data in-the-wild – a few images or videos taken on a monocular uncalibrated camera. The task is ill-posed and combining quality with real-time inference is challenging. That said, photo-realism is not achieved yet. 
17
We follow StylePeople [36] architecture as a baseline for this thesis. It is based on deferred neural rendering, as described above (see Figure 2.2b). The input of a neural renderer network is a rasterized image of a generic human body mesh, that does not contain any personal details, such as facial features, hair, clothes. It is rasterized with a neural texture tensor, that has usually between 4 and 16 channels. Feeding this ”neural” image through the renderer, the personal features are filled in, and a colorful image is returned, along with body contour segmentation. Both neural renderer and neural texture are jointly optimized using image-based losses (more details in Section 2.3), including an adversarial loss using a discriminator network (see the paragraph on GANs above). 
The neural renderer’s architecture is a CNN called U-Net [79], consisting of an encoder and decoder part. In encoder, input is gradually processed with convolutional down-scaling filters. Next, passing through the decoder, the data and the corresponding encoder layer’s activations are concatenated and further processed with convolutions, gradually up-scaling to the original resolu tion. The U-Net’s output is fed through two independent CNNs, predicting color and segmentation images that make up the final RGBA image. Commonly to improve training, the encoder part is replaced with a powerful CNN backbone, in our case ResNet[23, 37]. The discriminator’s archi tecture is exactly as in StyleGANv2 [51], the details of which are irrelevant for this work. 
The generic human body model used in StylePeople is Skinned Multi-Person Linear Model - Expressive (SMPL-X [74], derived from [62, 21]). It allows representing a variety of human bodies, including expressive faces and hands as 3D triangular meshes with N = 10475 vertices, and K = 54 joints for limbs, face and fingers (See Figure 2.2a). It does so by learning bodies distribution from many 3D scans of people. A particular body shape is derived as a linear combination of learned body blend shapes S = [S1, ..., S|β|] ∈ R3N×|β|, where β is a vector of linear coefficients. The body blend shapes are orthonormal principle components of the learned mesh vertex displacements. Typically, about 10 body shape components are enough for reconstruction of any body constitution. Face vertices have their own blend shapes E = [E1, ..., E|ψ|], combined with ψ coefficients, similarly about 10 are usually enough. 
Body mesh can be posed using learned orthonormal componentsP = [P1, ..., P9K] ∈ R3N×9K of vertex displacement with respect to a pose. The pose is defined by axis aligned rotations of body joints θ (|θ| = 3K). It is then expanded into rotation matrices K × 3 × 3 using Rodrigues formula [77], notated further as R : R|θ| → R9K. The computed body shape offsets, face shape offsets and pose offsets are simply added to an average body mesh T¯ in a default pose θ∗. Formally, a posed triangular mesh Tp can be obtained as: 
Tp(β, θ, ψ) = T¯ + BS(β; S) + BE(ψ; E) + BP (θ;P) (2.7) 
BS(β; S) = X|β| n=1 
|ψ| 
βnSn, BE(ψ; E) = X n=1 
ψnEn BP (θ;P) = X 
9K 
n=1 
18
(R(θ)n − R(θ∗)n) Pn (2.8) 
SMPL-X also solves issues of mesh deformations and self-intersections, e.g. in poses with bent arms or rotated neck. However, it is computationally expensive to compute and may be skipped, since the neural rendering may handle these artifacts. SMPL-X also proposes an algorithm for optimization-based fitting of the body model to a single image, which is used in StylePeople too. 
  
  
(a) (b) 
Figure 2.2: (a) SMPL-X body model results. Joints are predicted in image space, then body shape and 3D pose are fit to maximize alignment with the source image. The final mesh is generic, lacking person-specific features as hair and clothes. (b) StylePeople data flow. SMPL-X body mesh is rasterized with the neural texture, the image is then refined by a neural renderer, to fill in the person’s appearance, clothes, hair. A discriminator network adds supervision to the renderer, by classifying generated and real images of the person. Images adapted from [74] and [36]. 
2.3 Metrics of image similarity 
For comparison of real and synthesized images, many metrics can be applied. Those that are differentiable and fast to compute, are usually used as training losses, by minimizing which the DNN learns how to generate images similar to the real. The next description will follow the notation: f - a fake synthesized image, r - a real image, fsegm/rsegm - their segmentation masks, Af/Ar - intermediate DNN layers activations when processing fake and real images respectively. 
Mean Absolute Error (MAE or L1 [89], ideal value 0) measures absolute differences between corresponding pixels. It simplifies the training process, by pushing pixels to equality, but does not capture many image degradation effects like blur, warping, etc: 
MAE(f, r) = 1NXN i=1 
|fi − ri| ∈ [0, +∞] (2.9) 
Mean Squared Error (MSE or L2 [89], ideal value 0) measures squared differences between corresponding pixels. In terms of image comparison, similar to MAE. However, it has smoother values distribution, and penalizes high differences much more. Sometimes such penalization is too 
19
harsh, even though perceptually the difference is very small. 
MSE(f, r) = 1NXN i=1 
(fi − ri)2 ∈ [0, +∞] (2.10) 
Learned Perceptual Image Patch Similarity (LPIPS [107], ideal value 0) is based on an ob servation, that image-based neural networks (commonly VGG [85], Pix2Pix [48], AlexNet [56]) tend to learn inside their weights structured patterns from images. Given a new image, activations of the network will contain a proportion of those patterns that can be found on the image. Thus, we could infer such network with fake and real images, and compare activations. If they are identical, then the original images are perceptually similar on a global level. The difference in activations is applied as a loss during training. The metric is computed as an average MSE distance between corresponding activations (also normalized by the tensors size to equalize contribution of deep layers). 
LPIPS(f, r) = 1LXL l=1 
1 
size(Afl)MSE(Afl, Arl) ∈ [0, +∞], L is number of VGG layers (2.11) 
Structural Similarity (SSIM [99], ideal value 1) metric is based on an idea, that perception of noise on an image is dependent on luminance (mean of image values µ), contrast (standard deviation of image values σ) and structure (covariance between images σxy). The metric is computed for two images from components, that measure luminance difference (l(f, r)), contrast difference (c(f, r)), structure difference (s(f, r))). Importance of components is also controlled by α, β, γ powers. 
l(f, r) = 2µfµr + C1 
µf2 + µr2 + C1c(f, r) = 2σfσr + C2 
σf2 + σr2 + C2s(f, r) = σfr + C2/2 
σfσr + C2/2 
SSIM(f, r) = [l(f, r)]α· [c(f, r)]β· [s(f, r)]γ ∈ [−1; 1] 
(2.12) 
where C1 = (K1L)2, C2 = (K2L)2, L is a range of values stored in pixel channels (usually it is 1 for channels in floating-point format, or 255 for integer 8-bit channel format), K1 and K2 are arbitrary constants to ensure numerical stability for near-zero denominator. SSIM is usually computed in a 11×11 sliding window and averaged over the whole image. It can detect degradation due to noise, but might be insensitive to blur and color shift. 
Multi-Scale Structural Similarity (MS-SSIM [100], ideal value 1) is an extension of SSIM, but contrast and structure terms are computed on multiple scales, i.e. the original images are pro cessed with a low-pass filter and downsampled by a factor of 2, a total of M times. All the computed multi-scale components are multiplied to the metric. The metric was shown to be the same or better than SSIM in detecting many types of distortions: 
MS-SSIM(f, r) = [l(f, r)]α·YM j=1 
[cj (f, r)]βj[sj (f, r)]γj ∈ [−1; 1] (2.13) 
Peak Signal-to-Noise ratio (PSNR [41], ideal value +∞) metric comes from signal process ing theory. It captures a ratio of the maximum value in an noise-free signal to the average noise 
20
magnitude (can be captured by MSE). The metric indeed helps to detect noise levels, but it is much less useful as image similarity measure than other metrics, such as SSIM. 
PSNR(f, r) = 20 · log10 (max(r)) − 10 · log10 (MSE(f, r)) ∈ R (2.14) Adversarial loss (GAN-loss [33], ideal value 0.5) is used as a part of Generator - Discrimi nator DNN training architecture. It measures how well the discriminator learned to distinguish real images from fake (by assigning probabilities of realism: 0 is fake, 1 is real). The generator’s task is to synthesize images that cannot be detected by the discriminator. Ideally, discriminator should report probability 0.5 on average, indicating that it essentially reports a random guess. 
Feature matching loss (FM [81], ideal value 0) is analogous to LPIPS, but the intermediate activations on fake and real images are taken not from external image network, but from discrimi nator network. The motivation is that the generator network will learn to produce images that are perceived as real by the discriminator. 
Dice loss (ideal value 1, [90]) is widely used in various pixel segmentation tasks to learn segmentation masks. It maximizes area of masks intersection and minimizes union of the areas (to prevent false positives in the predicted mask). The formula treats masks as sets of pixels where 
segmentation value is greater than 0. 
Dice(fsegm, rsegm) = 2|fsegm ∩ rsegm| 
|fsegm| + |rsegm|(2.15) 
2.4 Ways of speeding up DNN inference 
State-of-the-art DNN methods indeed achieve high quality results, but may also have tremen dous computational costs. As an example, the models of natural language processing may surpass billions of trained parameters (weights): Turing-NLG has 17 billion parameters [80], GPT-3 - 175 billion [22], Megatron-Turing NGL - 530 billion [87]. This number affects the amount of required computer memory, as all the parameters, computed values (activations) of intermediate DNN lay ers, and analytic gradients need to be stored. Secondly, sufficient computing power is needed to make training time more feasible. For instance, a computing cluster of 10000 GPUs, 285000 CPU cores was built in order to train GPT-3, still requiring more than a month of constant training [12]. 
After training, the amount of minimum computing power to perform DNN inference is gen erally a lot smaller, because we can load and compute neural layers one by one without storing intermediate activations. Nevertheless, inference may still require dozens of GPUs, a handful of memory and seconds to compute. Thus, a few research directions of Deep Learning are dedicated to reduce the required computing power, trying to preserve the similar model’s quality. 
21
One such approach is known as knowledge distillation. Its idea, is that the learned values of DNN’s parameters are not equivalent to knowledge of how to perform a task. In fact, by knowl edge we can consider any sufficiently good mapping from input data to the desired output data [39]. Thus, we could try to train the original model fully to yield great results, and then to train an other DNN ”student” model with much fewer parameters. Both models are inferred with the same input data, and the student model is supervised to output the same results as the original model. Mimicking intermediate activations of the original model is also an option to replicate even more knowledge. The simplified neural architecture of the student models helps to reduce computation time and overfitting. That is because with fewer parameters there is less capacity in the student to remember the training data, and also imperfections of the original model’s predictions additionally regularize the student [34, 10]. However, knowledge distillation adds up more research work, with experimentation on neural architectures of the student, the training routines and hyper-parameters. 
There is an adjacent group of methods, called neural architecture search. It aims to automat ically find a DNN architecture that would outperform hand-crafted architectures on the given tasks. The motivation is to both reduce the committal effect of selecting a sub-optimal neural architecture by researchers, and to also reduce the amount of parameters required to accomplish the same tasks [29, 75]. However, a single search step requires to train the current network, collect quality metrics and then update the architecture. The convergence may require dozens of steps. This limits neural architecture search to lightweight DNNs, and it is hard to apply for big neural architectures. 
Another option comes from an observation, that some activation values of DNNs may be less important to the overall output than the others. Such activation values usually have low variance with respect to different input data, i.e. they frequently yield similar values. The corresponding learned parameters could then be replaced in the architecture by constants, if the quality does not decrease much. Such approach is called pruning. Similarly to knowledge distillation, it allows to train a big model with the full capacity, which eases training. Then the obtained knowledge is squeezed to the lower number of parameters [60, 19, 30]. 
On a computer hardware level, DNNs are usually trained and inferred with weights and ac tivations represented with 16-bit/32-bit floating point numbers (also referred to as half-float or FP16; full-float or FP32 respectively). A quantization approach switches to integer number for mats, with lower memory usage, e.g. 4-bit/8-bit/16-bit integer numbers (INT4/INT8/INT16). The main motivation is that integer operations can be computed faster than floating-point ones, their physical circuits are smaller, thus decreasing device’s size, overheating, energy consumption [71]. The quantization approach relies on a fact, that a vector of floating-point numbers x can be approx imately expressed as 
x ≈ s′x· x′int = sx · (xint − zx) (2.16) 
22
where s′xis a floating-point scalar, x′int contains only integer values. Then they can be reformulated using an integer zero-offset zx, so that values in xint are mapped uniformly to range (0, 2b − 1), with b being bit-width of a number. Having a floating-point vector x, it is quantized as: xint = round(x/sx) + zx (2.17) 
Since many operations in neural networks can be computed using dot products of vectors, this allows to represent them in a quantized form. For example, a basic operation of matrix-vector mul tiplication with a floating-point weights matrix Wn×m and a bias vector bn. The output activation vector An could be computed from an input activation vector xm as: 
A = b + W x 
≈ b + [sw(Wint − zw)] · [sx(xint − zx)] = b + swsx(Wintxint − xintzw−Wintzx + zxzw). 
(2.18) 
The last two terms shown in blue can be pre-computed for the whole DNN, and merged with the bias vector b [69]. However, since there is still an extra overhead from computation of xintzw, specifically for weights it is common to use symmetric quantization, where floating-point values are mapped to (−2b−1, 2b−1 − 1) integer range, with floating-point 0 mapped precisely to integer 0, thus making zw = 0, and eliminating the overhead. 
Typically, a post training quantization (PTQ) is applied to weights and activations of a DNN, because a complete experiment can be quantized via a provided software tool, and automatically be sped up from quantized computations. On the other hand, if precision of the outputs is a priority, a quantization-aware training (QAT) [55] approach is used. Here architecture tweaks are required to do quantized inference and weights updates, with an idea to make the model learn to deal with the quantization imprecision. As a downside, it enlarges training time and adds issues with analytic gradients back-propagation through rounded values [18]. 
Usually, the uniform quantization is appealing for simplicity of hardware implementation. However, the quantization range defined by constants s and z may be selected too wide, e.g. min max range of values in a tensor with existing outliers. Thus, a lot of precision will be lost as different numbers will be mapped to the same integer value. Instead, the range can be selected to minimize MSE with respect to the original floating-point values [16, 76]. If hardware supports it, instead of defining per-tensor quantization constants, they can be defined per-channel for more precision. Alternatively, a Cross Layer Equalization [67] can be applied to re-parameterize quantized weights and biases, compensating the channels’ discrepancy of ranges, without adding runtime overhead. A non-uniform quantization can also be used to devote more precision to near mean values, and less to the outliers, but the hardware support of it is very limited [17]. 
Moving on, DNNs also suffer from computational inefficiency, when similar input data leads to computing identical activations of layers. The DNN’s architecture can be designed to cache 
23
and reuse these values. It can be applied to video stream processing or synthesis, where multiple consecutive frames have similarity [53]. Additionally, filters of CNNs may frequently contain repeated weights after being quantized. Thus, as a filter slides over the data, certain dot-products can be reused [38]. While having a potential to speed up a certain DNN architecture, it requires immense manual hand-craft, which slows down active research and experimentation. 
The last but not least appoach, is to design the neural architectures efficiently, i.e with a good trade off of inference speed and outputs accuracy. It is obtained by reducing the number of parame ters, applying only fast-to-compute neural layers, and composing them into a sparse computational graph. It leads to a better utilization of memory caches and computing cores of the hardware. Often, such models are implemented using low-level parallelized code, that increases the number of floating-point operations per second (FLOPs) even further. The notable examples are: Mo bileNetV1 [44], MobileNetV2 [82], MobileNetV3 [43], ShuffleNetV1 [108], ShuffleNetV2 [65], CondenseNetV1 [45], CondenseNetV2 [105], EfficientNet [91]. Although the original papers ob tain similar or better results with their proposed architectures, the drop-in replacement is not guaran teed for every task. In this work a significant drop of quality is observed, when a baseline ResNet18 backbone was replaced with either MobileNetV2 or EfficientNet. 
2.5 Computations on mobile hardware 
Traditional desktop computers have hardware, where components are designed to be inde pendent and replaceable. This is achieved through standardization of connectors and signaling. This design is convenient when physical size of devices, power usage, or necessity of active cool ing are not issues. On the other hand there is mobile hardware, which can be found in modern smartphones, watches, AR and VR devices. There exists a strong desire to minimize physical size, weight, battery power usage and heat generation. Despite the wide variety of mobile devices and vendors, the state-of-the-art mobile architectures look similar on a high level. They are presented as Systems-on-a-chip (SoC), also referred to as heterogeneous platforms, where computing de vices of different underlying principles are fused under a single silicon. The computing work is offloaded to the designated computing devices, to minimize idleness and latency. Such platforms may accelerate general computing (CPU), graphics computing (GPU), signal processing, network ing, cryptography, external devices, camera processing, AI computing, sensing, etc. To name a few SoC platforms, there are: Nvidia Tegra [3], Samsung Exynos [6], Qualcomm Snapdragon [5]. Notably, all of them advance towards providing special computing devices for AI, and DNNs in particular [26]. Such AI accelerators can come by the names: Deep Learning Accelerator, Neural 
24
Processing Unit (NPU), Tensor Processing Unit (TPU), etc [101]. This work by default targets devices running Qualcomm Snapdragon SoC. 
The latest chip called Qualcomm Snapdragon 888 consists of an 8-core CPU, a GPU with mixed precision AI inference support (vectorized half-float/full-float), a sensing processor, and a Digital Signal Processor (DSP). The latter is especially optimized for parallel integer computations, which are common in real-time 1D or 2D digital signal processing (e.g. Fast Fourier Transform algorithm). It supports parallel scalar, vector and tensor operations, while retaining state-of-the-art low level of power usage. Besides, it is possible to use DSP as an AI accelerator to infer a quantized DNN, and get massive benefit in inference speed and energy consumption. All accelerators of the SoC are located on the same chip and have access to shared memory, allowing to reduce overhead of data transfer (which is common in desktop systems). The claimed performance of the combined accelerators is 26 TOPS (Trillion Operations per Second). The platform also offers a triple-core chip for multithreaded camera processing with up to 2.7 Gigapixels per second of throughput [95]. 
Importantly for this work, Qualcomm Snapdragon’s DSP works with tensors layout called BHWC (Batch-Height-Width-Channel), that is, the data of a single spatial element (pixel) is stored contiguously (e.g. C values for a pixel 1 are followed by C values for a pixel 2, and so on). This will be utilized during real-time input data generation for the DNN researched in this work. The other possible format is BCHW (Batch-Channel-Height-Width), which is usually used in software libraries for DNN training on desktop computers. 
As well as usual quantization approaches described in Section 2.4, for Snapdragon DSP a data-free quantization algorithm is implemented, which does not require estimating quantization ranges from example input dataset [68]. It features high quantization speed, which used to be critical with big datasets, and also achieves near-identical outputs’ quality for quantized DNNs. 
An AI model can possibly provide software level improvements for better hardware utiliza tion. For example, the work [72] suggests a pipeline approach, where a model is split into several stages. Instead of processing input from start to finish, each stage receives new input as soon as it finished the previous calculation. By combining usage of multiple accelerating devices on mobile hardware, it is possible to almost double real time performance. 
25
Chapter 3 
Methodology 
This chapter describes implementation of the mobile Augmented Reality (AR) pipeline of avatars rendering, and experiments on increasing the DNN’s [36] images quality for AR scenario. Section 3.1 lists target hardware and software, including the used libraries, tools, and configuration of the baseline model. Section 3.2 describes the mobile pipeline and algorithmic tricks used to achieve real-time performance. Section 3.3 presents motivation and implementation details for experiments with the DNN’s training procedure. 
3.1 Experimental setup 
The baseline DNN [36] model can be trained on a monocular video sequence of frames with a single person shown from multiple sides. There are several pre-made training video sequences with different people at our disposal. They were captured for approximately 1 minute on a station ary monocular smartphone camera, in resolution 1920 × 1080 pixels. Only each 8th frame from the video was retained to get a representative set of distinct body poses. Then, for each frame a segmentation mask was predicted using an external pretrained segmentation model Graphonomy [32], to replace color of background objects with 0 values. Next, SMPL-X [74] body model was fit to all the frames, i.e. predicting pose vector θ, facial expression vector ψ, and body shape vector β. The latter is purposely predicted to be the same across all the frames, in order to bias the DNN to synthesize images for this body shape only, which will be exploited in real-time inference on mobile in Section 3.2. Using body model fits, a posed body mesh for each frame can be obtained as was described in Section 2.2. 
Sometimes, 3D model fits may be misaligned with complicated ground truth 2D frames, e.g. due to complex twists and bending of joints, occluded body parts. To reduce the effect of training the DNN on outlying unstable inputs, many of the falsely fit ground truth images were manually filtered out from the training set. It was observed, that the quality of body model fits is critical for the avatars final quality. Hence, for the scope of this thesis, we decided to take a single training sequence, that has the best body model fits we could obtain. The training pipeline adjustments 
26
discussed in Section 3.3 will be primarily applied to this training sequence. This also allows the fair quantitative and qualitative comparison. For simplicity, when we further mention the training sequence, we mean this single training sequence. A few promising experiments will be repeated with a few other sequences to verify stability of results (for example, see Figure B.33). 
After filtering out the bad frames, the training sequence contains 410 samples. Out of them we holdout 13 frames as a validation set. It includes 11 consecutive frames from a frontal view, in poses that do not resemble any other training data; and also 2 frames with side and back view. As for a test set, we made a sequence of continuous difficult poses and camera angles. It includes an animation with extensive hands gesturing, a view on the avatar from top, bottom, far away, and close-up to the face. The test sequence is completely unrelated to the training data and was designed by us to verify that the DNN is capable of rendering frames that may be encountered by a user in an AR session. The possible visual artifacts may include flickering between two similar frames, clothes shaking, color shifts, blurriness, etc. See Appendix Section B.3, featuring a few training and test images that we use. Lacking an absolute metric of visual quality, in this research we mainly rely on manual inspection and comparison of the test video sequences. 
In the next paragraphs we will elaborate more on the specific configuration of the baseline DNN architecture, that was first mentioned in Section 2.2. By default, we stick to 512×512 pixels resolution of synthesized images. Both the ResNet18 encoder and decoder of the neural renderer consist of 5 levels of 2-layer CNNs with stride 2 and kernel sizes either 3 × 3 or 5 × 5. The U Net yields a tensor with 64 channels and the original spatial resolution. Two independent CNNs with 2 layers each (also called heads) then map it into a 3-channel RGB image, and a 1-channel segmentation mask, that are concatenated into the final RGBA avatar image. 
The neural texture is a tensor of 16 channels and resolution 512 × 512 by default (although may be arbitrary). For the SMPL-X body model mesh, there is a known mapping from 3D mesh vertices to the same 2D texture image coordinates (as on Figure 1.1), regardless of body shape (tall, short, slim or obese). In addition to the original resolution, extra texture scales of size 16 × 2k × 2k, ∀k ∈ {3, .., 8} are created and jointly optimized. This set of scales is referred to as a multi-scale neural texture. Before rasterizing the body mesh, all the scales are upsampled to 512 resolution and averaged into a single texture. The multi-scale texture allows to share information among adjacent vertices and proven to reduce flickering of the output avatar images. Also, the baseline neural texture uses a special initialization. It is computed as spectral coordinates of vertices on the graph, that is made out of the body mesh. In short, the graph’s Laplacian matrix is calculated, and Eigenvectors of it are called spectral coordinates [106]. A few first dimensions of these vectors are used to initialize the neural texture. The purpose of such initialization is in separation between different parts of the body (see Figure B.1), e.g. there are channels that assign opposite values to 
27
left/right limbs, or lower/upper body, or adjacent fingers and so on. Such initialization helps to guide the neural renderer to uniquely distinguish the body parts, improving convergence speed. 
During training, the synthesized image and its segmentation mask are supervised with a weighted sum of training losses. By default Dice, L1, LPIPS (VGG [85] based), Adversarial and Feature Matching losses are used, with weights 25, 25, 20, 1, 1 respectively. The adversarial loss is a Binary Cross Entropy (BCE) loss of predictions made by an ensemble of N = 3 discriminator networks. Each discriminator is a CNN that gradually decreases the spatial resolution and increases the number of channels, until finally a Sigmoid function returns a probability in range [0; 1] of the image being real. The discriminators are supervised to improve their accuracy on both synthesized and real images, while the neural renderer is supervised to decrease discriminators’ accuracy for the synthesized ones. See Formula 3.1 for BCE, when computing it for a real image x, then y is set 1 for discriminators; if x is a generated image, then y is 0 for discriminators, but 1 for the neural 
renderer. 
BCE = −1NXN i=0 
y · log2(Di(x)) + (1 − y) · log2(1 − Di(x)) (3.1) 
Adam [54] optimizer is utilized for training of the neural renderer, neural texture and dis criminators. They are optimized with learning rate (LR) of 10−3, 10−3and 10−5respectively. By default, the Adam’s first and second momentum values are 0.9 and 0.999 respectively, without weight decay (also known as L2 penalty on norm of the optimized parameters). 
In the baseline training process, a batch of random training samples is selected. Using body model’s mesh, a bounding box is found that tightly encloses the person’s extents on the ground truth image. The image is cropped by this bounding box using bilinear interpolation to get a 512 × 512 image with the person’s body in the image center. With the same resolution and alignment, the body mesh is rasterized. Then, both the ground truth and rasterization are processed with image-space augmentations (see Figure B.12), that perform random affine rotation, scale and translation of the image to improve generalization of the neural renderer to different camera views on the same avatar. Then the neural renderer is infered with input rasterization, and loss is computed between real and synthesized images. The gradient is computed and applied to all the learned parameters (the neural texture, neural renderer and discriminators) on every single batch. We train the pipeline on desktop hardware – one GPU NVIDIA Tesla P40. It takes about 16 hours to complete a single experiment of 500 epochs on about 500 frames of ground truth images, with batch size 8 and resolution 512×512 pixels. All training computations are carried out in full-float FP32 numbers format. As validation metrics, we compute LPIPS (AlexNet [56] based), SSIM, MS-SSIM, and PSNR, as well as tracking values of L1, Adversarial and Dice losses on the validation set. 
28
The experimentation on deep neural networks is carried out in Python programming language and PyTorch framework for the DNNs training. We use many Python libraries for data processing and visualization: NumPy, Matplotlib, OpenCV, Pillow, Open3D. In order to optimize parameters of the neural texture, it needs to be rasterized with the body mesh using differentiable rendering algorithm, we use Nvdiffrast library for that [57]. The image-space augmentations need to be differentiable for the same reason, for which we utilize Kornia library [27]. 
The primary mobile hardware that we target with our mobile application is SoC Qualcomm Snapdragon 888, which is one of the latest high-end platforms that features a fast and power effi cient DSP computing unit for AI, and high throughput for processing mobile camera data in real time. Although the preceding versions of the platform also contain the DSP, they are too slow to infer the baseline DNN in real-time. Qualcomm Snapdragon 835 is the oldest platform that we succeeded to launch the mobile application on, although with unacceptable performance. 
We develop the mobile application in Java and Kotlin programming languages. The devel opment environment consist of the Java Software Development Kit (SDK) and Android SDK. We perform installation of the compiled code on a mobile device via Android Debug Bridge (ADB) software utility. In order to launch the PyTorch DNN on mobile, first of all we use Snapdragon Neural Processing Engine SDK (SNPE, [1]). It includes Android runtime libraries, as well as desk top software tools for conversion of DNN architectures into binary files, that can be loaded onto mobile devices and executed. Among the tools there is a utility for post-training quantization of a neural architecture. We supply it with 5-10 input frames from the test sequence. The neural ren derer network is inferred with those frames, and the range of quantization is computed for each intermediate tensor. Then all the weights are quantized as was described in Section 2.4. The di rect conversion of PyTorch DNN models into SNPE format is not supported, however there is an Open Neural Network Exchange format (ONNX, [2]), which supports conversion from PyTorch to ONNX (provided by PyTorch), and from ONNX to SNPE (provided by SNPE SDK). The final file with the quantized DNN is passed along with the quantization constants to the mobile application to accordingly generate quantized input frames. 
On the mobile application’s side, we use SNPE Android Runtime library to load the model’s file and to specify performance profiles. Those control how much of computing power should be devoted by the device. We use ”Sustained High Performance” profile, since we intend to continu ously infer the DNN over a long period of time with low latency. On the other hand, there is the absolute highest performance profile, that would lead to the best performance over a short period of time, that would drastically decrease upon reaching the overheating limit. Prior to inference, SNPE should be given with pointers to memory locations, where to read the input data, and where 
29
to write the output data. We accordingly preallocate buffers on CPU and GPU, and use them for the whole run of the mobile application to minimize costs of memory allocation on each frame. 
For the DNN inference, we need to generate an input frame (see Figure 2.2b), that would show the body mesh oriented as an object of Augmented Reality. Hence, the orientation should correspond to the mobile device’s camera. This requires continuous tracking of camera position in the real world, as well as positions of the surrounding environment. We use ARCore SDK [8] that provides it with little programming required. It uses inertial measurement units, camera images and depth sensor of the device to keep track of distinct points in the camera view. This allows to estimate proximity of the real world objects, and to reconstruct vertical and horizontal flat surfaces, such as floors, desks, walls. The location and bounds of these surfaces can be queried from the ARCore. It also allows to place and track anchor points on the detected surfaces. The tracking information can be retrieved from ARCore in a form of transformation matrices of the anchors and camera (extrinsics, as in Formula 2.5). The projection matrix of the camera (intrinsics, as in Formula 2.4) can also be retrieved, allowing to render images that span the whole screen of the device. 
The generation of input frames is in essence a classic rasterization of a triangular mesh onto an image using a 2D texture. This process can be delegated to the high-performance graphics library, which is called Open Graphics Library (OpenGL, [9]). It provides low-level control of a GPU device, to layout and are pre-process the data of meshes and textures. Then, using GPU programs written in OpenGL Shading Language (GLSL, [4]), also referred to as shader programs, we can make linear algebra computations on individual vertices of the mesh (vertex shader), or individual pixels on the rendered image (fragment shader). Since all those primitives can be processed inde pendently, this computation is automatically parallelized on GPU by OpenGL, thus achieving very high performance of rasterization. We also use it to render camera frames on the screen. On top of them we render the avatar images, resulted from the DNN’s inference (see Figure B.16). In fact, we use a subset of the library, called OpenGL for Embedded Systems (OpenGL ES) version 3.2, that consists of core functionality for rasterization, lacking many high-end graphical extensions. 
The application is compiled with all performance optimizations enabled. The frame time of the mobile AR experience is measured directly in the application. We report timings of processing steps of the current frame via logging, namely: time of SMPL-X body mesh inference from a pose vector, time of rendering the DNN’s input frame on GPU, time of delay between obtaining this data and receiving it on the DNN’s side, time of DNN’s inference, time of rendering the inferred outputs on the screen. However we notice, that by far the highest processing time is taken by the DNN inference step. To measure its timings more accurately, we created a separate mode for the application, which we call ”benchmarking”. In this mode, the only thing that is being calculated is continuous inference of the DNN for a few minutes with a constant random tensor of data. Thus, 
30
the computing device will be maximally utilized with the DNN inference only. We consider it an upper bound of the power consumption and heat generation. The benchmarking reports increase in temperature and average timing of DNN inference. We additionally use Qualcomm Snapdragon Profiler software. It allows to capture hardware metrics of a running mobile application. From that, we visualize occupation of computing devices over time, and reason about potential bottlenecks. 
3.2 Mobile application development 
The Android mobile application was developed from scratch specifically for this Master the sis. A considerable amount of technical work was spent on implementing the general functionality of the application. This includes: 
• system events handling, such as opening, closing, resuming from the operating system; • requesting of system permissions on camera and hard drive storage; 
• adding binary files into an installation package, including runtime libraries, files of quantized DNN models, neural textures, configuration files, assets for inference of SMPL-X, etc; • setup of logging; 
• reacting to user interaction, such as finger taps, swipes, pinch movements; • populating the user interface with options to select a particular DNN for inference, options of DNN input generation, predefined animations, features of rasterization, etc. 
We publish only a part of the code base which is the most relevant to synthesis of avatar images on mobile, that is described in the following paragraphs (See Appendix A). 
We initialize OpenGL library, by creating a Surface object from the Android SDK, that spans the whole screen of the device, and specifying it as a context of OpenGL to render to. This creates a separate execution thread on CPU. Only on this thread the commands of OpenGL (and thus GPU processing) can be executed. We configure the context to use 8 bits for all color channels of the screen and 32 bits for values in the Z-buffer. In OpenGL all the data is allocated on GPU as contiguous arrays, and the library has to be programmed how to interpret these arrays. The triangular meshes can be defined by a flat array of vertex attributes, where consecutive elements contain attributes of a single vertex. The most basic scenario is to pass an array of ⟨x, y, z, u, v⟩ with xyz being a 3D vertex position in a local coordinate space of the modeled object, and uv being texture coordinates of the vertex. Also, a connectivity array has to be provided in a form of indices in the array of vertex attributes. Three consecutive indices form a single triangle. We use back-face culling throughout the rendering, meaning that OpenGL will not render triangles facing away from the camera, slightly improving performance by culling some triangles out of 
31
processing. To define which triangles are facing away, we specify to interpret vertex indices as clockwise triangles, meaning that a triangle of indices ⟨a, b, c⟩ will face towards the camera if and only if after projecting on the screen the vertices appear in a clockwise order. Overall, we use three separate triangular meshes, one for SMPL-X human body mesh (described further), and two ”meshes” which are essentially rectangles. The first one is rasterized, spanning the whole screen, and taking color values from a texture that contains a current camera frame. The other one is similarly rasterized, but at a location of the avatar, taking values from a texture loaded with output of the inferred DNN (See a debugging illustration from the mobile application on Figure B.17c). 
OpenGL doesn’t support storage or rasterization of images with more than 4 channels (Red Green-Blue-Alpha only). We need to load the neural texture of C channels on GPU. If it is a multi scale texture, we convert it into a single scale, by averaging all the scales into the C × 512 × 512 texture scale. Then we split it by 4 channels and load these parts as separate arrays on GPU. However querying of multiple textures requires extra processing time in OpenGL fragment shader programs. We thus create an OpenGL’s Array Texture data structure, that improves querying of multiple textures with the same texture coordinates, which is exactly how we use these texture parts. We use FP16 format for storage, to a reduce memory footprint of the texture compared to FP32. This also should result in better utilization of texture memory caches inside GPU. OpenGL allows to apply an additional filtering algorithm on textures, to generate multiple scales of the texture, which are called MIP-maps (see Figure B.6a). Those can be used to render triangles that span very little area on the screen. The filtering prevents blurriness and aliasing artifacts from appearing due to bilinear sampling. Furthermore, anisotropic filtering can be applied to also generate stratched variants of scales. It improves quality of rendering triangles that face at steep angles to the camera (see Figures B.6b and B.7). Both methods require extra texture memory: about x2 for MIP map ping, and up to x16 for anisotropic filtering, and slightly more processing time. However, given the small resolution of our textures and relatively low triangle count in the body model, we found the performance difference to be negligible. We would like to clarify the confusing terminology. The multi-scale texture exists only in the DNN pipeline in Python, the scales of it are upsampled and averaged before rasterization. The texture MIP maps in the mobile application always remain at lower resolutions, and a GPU driver heuristically selects an appropriate MIP-map to render a certain triangle. 
Next we initialize a tracking session of ARCore library. It starts intercepting and processing the camera frames on GPU, allowing to refer to the camera data as an OpenGL texture. By default, ARCore processes a new camera frame and awaits for the next to arrive. The application is then capped to the camera’s update rate – 30 frames-per-second (FPS) for our devices. We configure 
32
ARCore to never wait and process the current camera frame, regardless if it was already processed. This allowed us to cap the application to the Android’s internal limit of 60 FPS. 
As was mentioned in Section 3.1, we initialize SNPE DNN models with ”Sustained High Performance” profile, and the computing unit specified by the user (supporting either GPU or DSP). GPU offers better support of neural layers used in DNN models, since it can perform computations in FP32 or FP16 format, analogously to a desktop GPU. But running a DNN on it would also mean that input rasterization and DNN inference would have to be done one after another, possibly bottlenecking the performance. On the other hand, DSP offers much higher performance than GPU, and inference can take place independently and in parallel with our input generation on GPU. However, the usability of DSP is very limited, e.g. much fewer neural layers support quantized computations; also loading of the DNN models requires long initialization, up to several minutes. Besides, Qualcomm Snapdragon’s DSP contains a hardware security policy, that only allows to run on consumer devices the programs, that were certified by the hardware vendor. We ignore this issue for our project, by having test engineering samples of smartphones that suppress this security check. If SNPE fails to load a model on DSP, it may fallback to using GPU, or even CPU execution, obviously being extremely slow for inference. 
Finally, before inference, we can pre-compute parts of the SMPL-X [74] body mesh. Since it’s only done once, we do this computation on CPU. We load 3D positions of N vertices in the template mesh with a rest body pose. Then, following the algorithm (see at the end of Section 2.2), for mobile inference we’ll use the same body shape vector β that was used during DNN training. From β we can compute a linear component BS(β; S) as in Formula 2.8, that offsets the template vertex positions to represent the specific body constitution of the avatar. We compute it is as a multiplication of a tensor with shape N × 3 × |β| (provided by SMPL-X) and the β vector (from the dataset), yielding the offsets N × 3 that we add to the template vertex positions. We similarly compute the BE(ψ; E) linear component, that offsets vertices of the face. Although the face shape can slightly change between different facial expressions, we only compute the linear component once before inference, for simplicity. Lastly, SMPL-X includes a pretrained linear regressor of joints 3D positions, from all the mesh vertices. We also pre-compute this regression, which comes down to computing multiplication of the regressor’s matrix 55 × N and the mesh vertex positions N × 3, yielding the 3D positions of 55 body joints. 
During inference, the pose-dependent linear component has to be computed and added for each frame in order to get a posed body mesh. As was already described, it involves finding a weighted sum of 55 affine transformation matrices of size 4 × 4, with a separate set of weights for each vertex. The weights define how strong the rotation of a particular joint affects offset of the vertex (for example, see Figure 3.4, how some vertices are dominantly affected by a particular 
33
nearby joint). Having approximately 10 000 vertices in the mesh, we need to multiply and add a total of 55·10000·4·4 = 8800000 floating-point values on every frame. This computation would be time consuming on CPU, but since it can be trivially parallelized, we instead pre-allocate the array of weights in OpenGL (on GPU), and compute the weighted sums in the vertex shader program. We further noticed, that among 55 weights per vertex at least 43 are zeros. Thus we can improve the performance, by sorting the weights on CPU during the pre-processing step, and allocating on GPU only 12 most significant weights and their joint indices i ∈ {1, ..., 55}. Overall, we load on GPU the pre-computed shaped vertex positions ⟨x, y, z⟩, texture coordinates ⟨u, v⟩, joints weights wk and joints indices ik as an array of vertex attributes on GPU. We use an interleaved layout, where ⟨x, y, z, u, v, w1, .., w12, i1, .., i12⟩ of a single vertex are located contiguously in the GPU array, improving memory cache efficiency of posing a single vertex. 
Next, we will describe the interactive stage of the application, which is an infinite loop of computing and rendering the AR experience. Each frame starts by inferring the pose-dependent linear component BP (θ;P) of SMPL-X. It is computed from a pose vector θ, containing 3 · 55 values with axis-aligned rotation vectors for all the joints. A single rotation vector r = ⟨rx, ry, rz⟩ needs to be converted into a 3 × 3 rotation transformation matrix R, using Rodrigues [77] formula 
(we use I for an identity matrix): 
R = I3×3 + sin(∥r∥) · A + (1 − cos(∥r∥)) · A2, where A =1∥r∥ 
 
0 −rz ry 
rz 0 −rx −ry rx 0 
 
(3.2) 
The joints apply their rotations and translations recursively, e.g. a position of a finger can be found from sequentially applying transforms of the arm, forearm, palm, and every knuckle. We could compute each joint’s final transformation independently, by multiplying all chains of the matrices. However, we can exploit the fact, that the joints form a tree structure, i.e. every joint depends on only one parent joint without loops. Thus, we use a topologically ordered array of 55 joints, meaning that no joint in the array precedes its parent joint. This allows faster CPU computation for 4×4 transformation matrices of joints, because by the time we process a joint i, all its dependent transformations are already computed in a form of a single matrix of its direct parent. We take 3D locations of joints t (regressed during the initialization stage), and get relative joint translations ∆t by subtracting joints positions from their parents’. The final recursive transform H 
for a joint i is found as: 
Hi = Hp · 
 
Ri3×3 ∆ti3×1 01×3 1 
 
 · 
 
I3×3 −ti 01×3 1 
 
 (3.3) 
where p is a parent index, the transformation matrix Hp of which is already found by the time we compute Hi. The first matrix multiplication computes a relative recursive transform for the joint, and the second multiplication subtracts the true posed location of the joint. From the programming 
34
point of view, to save on computations, we implement the matrix multiplication operations not naively, but by expanding and merging the operations together, e.g. computing each element of R matrices in place. We try to iterate the data in memory in a cache-friendly way, to write the data consecutively. We expect that it helps to retain exclusive usage of cache lines, without overhead of cache synchronization. For the full SMPL-X inference implementation, head to Listing A.6. 
Then, we extract tracking information from the ARCore session, namely camera intrinsic and extrinsic matrices, as well as transformation matrices of the tracked anchors. We pre-multiply the anchor’s matrix by camera extrinsic matrix, and then by the intrinsic matrix, obtaining a single transformation from local body mesh coordinates to projected image coordinates. Unlike the classic representation of intrinsics, as in Formula 2.4, OpenGL uses an equivalent representation, but with another notation, called Native Device Coordinates. Formerly, the projection frustum was defined with a 3×4 matrix, parameterized with image width and height W/H, focal lengths fx/fy from the pin-hole to the image, and image center offset cx/cy in pixel coordinates. In OpenGL, the frustum is set by a 4 × 4 matrix via 6 distances that bound the frustum: top t, bottom b, left l and right r planes; a near plane n, analogous to focal distance; and a far plane f, which defines the maximum depth that is mapped by the virtual camera. See Figure 3.1 illustrating the meaning of the frustum parameters. The matrix has 4×4 size in order to additionally compute depth of each pixel, allowing to store it in a Z-buffer, and querying if some rasterized pixels are being occluded. We can represent 
the same projection with either notation: 
 
 
 
 
2fx 
W0W−2cx 
W0 
 
OpenGL 
2n 
r−l0r+l 
r−l0 
 
(3.4) 
KNDC 
previous notation = 
02fx H 
H−2cy 
H0 
notation = 
02n t−b 
t+b 
t−b0 
0 0 −f+n f−n 
−2fn f−n 
0 0 −f+n f−n 
−2fn f−n 
0 0 −1 0 
0 0 −1 0 
There are a few changes required for this projection matrix. Since we use it to rasterize an input frame to the DNN, we have to supply square images (512 × 512 pixels resolution). However the default projection matrix has aspect ratio of the device’s screen. The frustum can be squeezed, e.g. along vertical y axis, by multiplying the second row of KNDC by the screen dimensions ratio W/H. Additionally OpenGL stores images from a bottom row to top, meaning that it appears vertically flipped when we use it outside OpenGL. To solve it, the element at second row and second column can be negated. 
Now that we have posed joints, in order to pose all the mesh vertices we send the trans formation matrices Hi, as well as the merged matrix of extrinsics and intrinsics on GPU, as uni form1 variables of the vertex shader program (see Listing A.7). Here, for each body mesh vertex we combine the most significant joints weights with corresponding matrices, taken by index off 
1The term uniform is specific to OpenGL, it means that a single variable value is shared across all the primitives (vertices/pixels) processed by a shader. 
35
  

Figure 3.1: A frustum of the pin-hole virtual camera model. It is a pyramid, where apex is both the camera position and the origin of coordinates. x and y axes align with the camera’s right and top directions respectively, z axis faces straight in the front (in some notation straight behind). An image is virtually placed at a z = n coordinate (referred to as near plane, or focal distance), parallel to the xy plane. The frustum’s extents along x (y) axis are specified with left l and right r (bottom b and top t) parameters. An image is formed by projecting the frustum’s content on the near image plane. It is common to limit the frustum with a far plane f, so that a finite-bit representation can be used in a computer to store depth information used for occlusion testing. Image from kor.faithcov.org/749684-gl-modelview-and-gl-projection-WBGZIU 
set specified in the vertex attributes. Notice, that when we compute uniqueVertexTransform matrix, we don’t add up the matrices plainly like a + b + ... + g + h, instead we use a scheme ((a + b) + (c + d)) + ((e + f) + (g + h)). Due to the order of operations execution, the former variant requires to compute a + b, and only after that compute (a + b) + c and further, even if hardware has multiple Multiply-Accumulate circuits (MAC) available. With the latter variant, we force computation of 4 inner sums independently, and then add them into the final matrix. This little reordering can optimize the performance, since its effect scales with the number of vertices. The vertex shader terminates by transforming the vertex with the affine transform, and projecting it into the screen coordinates. 
Then OpenGL automatically performs rasterization into an image. OpenGL interpolates ver tex attributes and passes them into a fragment shader program, which is executed for all image pixels (see Listing A.8). We use interpolation of texture coordinates, to sample the neural texture in agreement with the texture unwrapping, and construct the pixels of the DNN’s input tensor. SNPE processes the tensors in Batch-Height-Width-Channel layout (BHWC), i.e. all channel val ues of a single pixel are followed by channel values of the next pixel. On the bright sight, OpenGL uses exactly the same layout for rendered images. However once again, we stumbled upon a limita tion of OpenGL that it cannot generate images of more than 4 channels, while we need C channels. One solution is to render ⌈C4⌉ images of size 512 × 512 × 4 using OpenGL on GPU, and then to permute the data in CPU memory to make a single tensor 512 × 512 × C, and pass it to the DNN. 