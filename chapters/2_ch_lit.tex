\chapter{Literature Review}\label{chapter:lit}

This chapter introduces to traditional and Deep Learning approaches of image synthesis, and overviews ways of inferring DNNs in real-time on mobile devices. Sections \ref{lit:traditional-cg} and \ref{lit:nrender} gradually expand on state-of-the-art reconstruction of human images, with more attention to the parts that comprise the basis for this work. The used metrics and optimization losses are described in Section \ref{lit:metrics}. Then, methods for decreasing DNNs inference time are given (Section \ref{lit:dnn-speedup}) with a few peculiarities of the mobile devices targeted by this work (Section \ref{lit:mobile}).

\section{Traditional 3D representations and rendering algorithms}
\label{lit:traditional-cg}

Ever since the dawn of Computer Graphics, many primitive representations for 3D world were proposed to reconstruct real objects. The representations may model volumetric properties, such as occupancy, density of matter, radiance, color at a certain point of 3D space; or model surfaces and assign these properties to points on the surface. The definition of the primitives may also be explicit or implicit. An explicit surface can be defined by taking every point on a real coordinate plane $\mathbb{R}^2$, and mapping it to the third "height" coordinate explicitly via function $f_e(.) \in \mathbb{R}$. An implicit surface is defined by a zero-level set of a multivariable function $f_i(.) \in \mathbb{R}$ \cite{survey:advances-nn22}.
\begin{multicols}{2}
\setlength\abovedisplayskip{0pt}
\noindent
\begin{equation}
	\renewcommand\arraystretch{0.6}
	S_{explicit} = \left\{ 
	\left( \begin{array}{c} x \\ y \\ f_{e}(x,y) \end{array} \right) \Bigg\rvert 
	\left( \begin{array}{c} x \\ y \end{array} \right) \in \mathbb{R}^2
	\right\}
\end{equation}
\begin{equation}
	\renewcommand\arraystretch{0.6}
	S_{implicit} = \left\{ \begin{pmatrix} x \\ y \\ z \end{pmatrix} \in \mathbb{R}^3 \Bigg\rvert f_{i}(x, y, z) = 0 \right\}
\end{equation}
\setlength\belowdisplayskip{0pt} 
\end{multicols} 

Unlike surfaces, for volumes the values of its properties are usually defined explicitly for all points in the 3D space:

\begin{equation}
	\renewcommand\arraystretch{0.6}
	V = \left\{ f_{vol}(x,y,z)  \Bigg\rvert \begin{pmatrix} x \\ y \\ z \end{pmatrix} \in \mathbb{R}^3 \right\}
\end{equation}
  

Let's look at the common surface and volume representations in details. A \textit{point cloud} is a discrete set of points in $\mathbb{R}^n$ (usually $\mathbb{R}^3$). It can model either surfaces (thus each point belongs to the surface) or volumes. The points may be associated with additional attributes, such as: color, density, orientation vector (normal) of a surface. When rendering an image from a point cloud, usually the points are interpreted as spheres with certain radius, in order to compensate sparsity and lack of connectivity between adjacent points \cite{aux:pointcloud21}.

A \textit{polygonal mesh} is the most wide-spread 3D representation, being a compromise between granularity of 3D modeling and computational requirements to render adequate images. It reconstructs curvature of a surface using small planar segments, generally $N$-sided polygons, but usually triangles or quads. A set of 3D points (vertices) is defined, as well as information about which vertices are connected and in which order. Every vertex can be associated with additional attributes, but fine details of the whole polygons can be stored using \textit{textures}. Textures are commonly 2D images, containing in the pixels values of attributes (color, surface orientation, etc). Each 3D vertex on a mesh has designated 2D texture coordinates (also referred to as UV-coordinates; See Figure \ref{intro:fig:mesh-texture}). A barycentric interpolation of vertices' texture coordinates is used to sample attribute value from the texture for a certain point in a polygon. Textures allow to model details, that are too hard to approximate directly with polygons, e.g. small wrinkles, folds of clothes, complex color patterns, etc . Using textures, any information for polygons surface can be conveyed, e.g. by storing orthogonal direction for all points in polygons, interaction of light and very small details can be simulated without additional computational overhead \cite{aux:normal-mapping78}. 

A \textit{voxel grid} is commonly referred to as 3D image, for also being a regular grid of primitives - cubes (voxels) with associated attributes. As volumetric representations, they may store occupation or density of matter at all points in a closed 3D area \cite{aux:voxels98}. Voxel grids are fast to process, but are enormously large in memory. Storing fine-detailed geometry requires shrinking the grid's scale, but memory consumption grows cubically. Thus, attempts on sparse voxel grids are made, using hyerarchical structures, that are longer to query, but can encapsulate large empty space into very large voxels, and reconstruct fine details using very small voxels where needed \cite{aux:voxels-sparse13}.

\textit{Implicit functions}, as already described, can model surfaces. For example, we can define a Signed Distance Function for all points in a 3D volume. The values tell the closest distance to the surface, with negative sign if the point is "inside" a surface. However, to render an image, the surface (i.e. the zero-distance set) has to be queried repeatedly from the implicit representation. This makes image synthesis computationally expensive, unless an intermediate explicit representation is built to approximate the surface, e.g. using Marching Cubes algorithm \cite{aux:marching-cubes87} to reconstruct a mesh.

The computer graphics evolution led to computing hardware to support some 3D representations. For instance, images are now typically rendered using Graphic Processing Units (GPUs), that unlike general Central Processing Units (CPUs) implement operations of linear algebra in a massively parallel way, allowing to process millions of primitives in real-time. Next, we'll describe the most common rendering algorithms called \textit{ray casting} and \textit{rasterization}. They can be adapted to any 3D representation, although sometimes with too high computational complexity.

For both algorithms, a virtual camera is mathematically modeled. In physical cameras, light that bounces in the 3D space forms an image as it hits the camera's sensor. Light can be captured from a larger span of view using lenses, that focus light rays into a single spot, referred to as camera origin or focal point. For virtual cameras, a pinhole model is used, where a focal point is chosen, and the 3D scene representation is projected on a pixel grid, as if it was placed at a distance from the focal point, called focal length. From that, a view \textit{frustum} is defined, that is a pyramid that extends from the focal point through the image to infinity, and has apex cut off at the focal length. Objects outside the frustum are invisible to the virtual camera. See Figure \ref{lit:fig:rasterization}, where frustum spans from the image to infinity along the lines converging at the focal point.

The image formation can be mathematically expressed using a projecting transformation $\bm{\mathrm{K}}$ (also called intrinsic matrix, see Formula \ref{lit:eq:intrinsic}), parameterized by focal lengths along axes ($f_x$, $f_y$), image center point ($c_x$, $c_y$), and axis skew $\gamma$. This transform maps a 3D point, which is extended to 4D homogeneous space $\bm{\mathrm{p}} = [x, y, z, 1]$ to a 3D point $\bm{\mathrm{p_{proj}}} = \bm{\mathrm{K}} \cdot \bm{\mathrm{p}} = [x_p, y_p, w_p]$. Dividing by $w_p$, we get $\bm{\mathrm{p_{img}}} = [x_p/w_p, y_p/w_p, 1]$. The last coordinate can be discarded being equal to 1. Now $\bm{\mathrm{p_{img}}}$ contains pixel coordinates of the projected 3D point. This transform only simulates a camera at the coordinates origin. To place it at any point in 3D space, an additional affine transformation matrix \bm{\mathrm{E}} (extrinsic, see Formula \ref{lit:eq:extrinsic}) is introduced, which defines camera rotation \bm{\mathrm{R}} and translation \bm{\mathrm{t}} in the homogeneous space \cite{survey:advances-nn22}. The projection is thus $\bm{\mathrm{p_{proj}}} = \bm{\mathrm{K}} \cdot \bm{\mathrm{E}} \cdot \bm{\mathrm{p}}$ .

\begin{multicols}{2}
	\setlength\abovedisplayskip{0pt}
	\noindent
	\begin{equation}
		\bm{\mathrm{K}} = \begin{bmatrix} 
			f_x & \gamma & c_x & 0 \\
			0   & f_y    & c_y & 0 \\
			0   & 0      & 1   & 0 \\
		\end{bmatrix}
		\label{lit:eq:intrinsic}
	\end{equation}
	\begin{equation}
		\vphantom{\begin{bmatrix} a \\ b \\ c \end{bmatrix}}
		\bm{\mathrm{E}} = \begin{bmatrix} 
			\bm{\mathrm{R}}_{3 \times 3} & \bm{\mathrm{t}}_{3 \times 1} \\
			\bm{\mathrm{0}}_{1 \times 3} & 1 \\
		\end{bmatrix}
		\label{lit:eq:extrinsic}
	\end{equation}
	\setlength\belowdisplayskip{0pt} 
\end{multicols} 

Ray casting method uses the above math to project rays, that pass through all pixels of the image and converge at the focal point. At a point where a surface is hit by the ray, surface attributes are extracted from the 3D representation and the pixel is colored accordingly. To model more realistic lighting, the reflection from the hit-points can be calculated, thus attributes from a secondary hit will be accumulated. This recursive process is also referred to as ray tracing. Such approach is computationally heavy, as 3D representation has to be queried many times for each ray to detect hits. On the bright side, the computational complexity only scales with the number of image pixels, no matter the number of objects in the scene (see Figure \ref{lit:fig:raytracing}).

Rasterization is typically used together with mesh 3D representation \cite{aux:raster94}. It's much more widely used due to its computational efficiency and hardware support. Here, vertices of all polygons are projected onto the image. Pixels that belong to the projected polygons are colored. Additionally for each pixel, the nearest polygon's depth is stored as depth image (also called $Z$-buffer). This helps to resolve occlusion between polygons. Since it's usual that the number of vertices in the view frustum is smaller than the number of pixels, the computation is much faster than ray casting. However, it scales with the number of polygons, requiring to apply algorithms for efficient discarding of occluded polygons in real-time. Also, rasterization loses in producing realistic specular phenomena, such as shadows, due to lack of physical image formation modeling.



%\begin{figure}[h!]
%	\centering
%	\includegraphics[width=\linewidth]{\imgfp/representations}
%	\caption{Different 3D representations. Voxels comprise uniform axis-aligned grid with occupancy information. Meshes define 3D points and their connectivity into 3D plane patches (polygons). Point clouds define just 3D points. Signed Distance Function for each point in 3D space associates a distance to a surface, points with distance 0 implicitly model a surface. The picture is adapted from \href{https://www.youtube.com/watch?v=r9lvAyAgR3E}{www.youtube.com/watch?v=r9lvAyAgR3E}}
%	\label{lit:fig:3d-representations}
%\end{figure}

\begin{figure}[h!]
	%\fboxrule=2pt
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[height=5cm]{\imgfp/raytracing}
		\caption{}
		\label{lit:fig:raytracing}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\centering
		\includegraphics[height=5cm]{\imgfp/rasterization-s}
		\caption{}
		\label{lit:fig:rasterization}
	\end{subfigure}
	
	\caption{(\protect\subref{lit:fig:raytracing}) Rays are cast from the camera origin through each pixel, and if a ray hits any surface, the pixel is colored accordingly. (\protect\subref{lit:fig:rasterization}) In rasterization, vertices of a polygon are projected onto a raster grid, filling the pixels that belong to the projected shape. Images from \href{https://www.scratchapixel.com/}{www.scratchapixel.com}}
	\label{lit:fig:rendering-methods}
\end{figure}

\section{Neural rendering and human reconstruction}
\alert{CNN, MLP universal function approximator, overfitting, losses, hyperparameters, backpropagation}
%	\item neural representation
	
\label{lit:nrender}
\begin{itemize}
	\item deferred neural rendering \cite{dnn:deferred19}
	\item NeRF \cite{dnn:nerf20,dnn:phorhum22}
	\item GANs, StyleGAN \cite{dnn:gan14, dnn:stylegan-v1-19,dnn:stylegan-v2-20,dnn:stylegan-v3-21,survey:gans:18}
	\item human avatars, from video, a few shots; monocular and not \cite{dnn:volumetric-primitives21}
	\item StylePeople in details \cite{dnn:stylepeople21}
	
	Avatar rendering can be described as a learned operator $f_\theta$, that takes as an input an image with $L$ channels and spatial dimensions $H \times W$, and outputs a Red-Green-Blue-Segmentation (RGBS) colorful image with the same spatial dimensions. The input image should be a *rasterization* of a generic human triangular mesh $M$, with $L$-channels texture $T$ (also called *neural texture*), given that the image is projected using camera parameters $C$. 
	
	The idea behind this structure, is that human mesh $M$ doesn't need to carry information about fine details, e.g. face features, loose clothes and haircut. Instead, such details of a particular person's appearance can be learned in the texture $T$ and parameters of renderer $f_\theta$. 
	
	The mentioned paper proposes ways to train a GAN that generates textures $T$, and a rendering DNN $f_\theta$ from a video sequence with a person. After the training, an arbitrary rasterization of the mesh with the prepared texture $T$ can be used. For example, the mesh's skeleton can be animated, and the avatar can be observed from an arbitrary camera pose. 
	
	\item unified body models (SMPLX) \cite{dnn:smpl15, dnn:smplify16, dnn:smplx19}
	
	The human 3D model, on which the neural texture is applied, is based on work by the name SMPL-X. This unified body model is used for construction of a human's triangular mesh with more than 10 000 vertices. SMPL-X also features detailed poses of hands and faces, and provides robustness to self-intersection of joints. The input parameters to create the 3D model are:
	\begin{itemize}
		\item  Shape parameters - values that carry meaning of body scale, incorporating height, width of hips, size of a head, etc.
		\item  Joints pose - values encoding skeleton pose, location of joints, hands, fingers.
		\item  Expression parameters - values encoding facial expression.
		\item  Gender parameter - value controlling creation of male/female bodies.
	\end{itemize}

	\item training data augmentations
	
\end{itemize}

\section{Metrics of image similarity}
\label{lit:metrics}
\begin{itemize}
	\item Structural Similarity
	\item Peak Signal to Noise ratio
	\item Perceptual Similarity
	\item Image L1 distance
	\item Adversarial loss
	\item Feature matching
	\item Dice loss
	\item LPIPS
\end{itemize}

\section{Inference time decreasing approaches}
\label{lit:dnn-speedup}

State-of-the-art DNN methods indeed achieve high quality results, but may also have tremendous computational costs. As an example, the models of natural language processing may surpass billions of trained parameters (weights): Turing-NLG has 17 billion parameters \cite{dnn:turingnlg20}, GPT-3 - 175 billion \cite{dnn:gpt3-20}, Megatron-Turing NGL - 530 billion \cite{dnn:megatron22}. This number affects the amount of required computer memory, as all the parameters, computed values (activations) of intermediate DNN layers, and analytic gradients need to be stored. Secondly, sufficient computing power is needed to make training time more feasible. For instance, a computing cluster of 10000 GPUs, 285000 CPU cores was built in order to train GPT-3, still requiring more than a month of constant training \cite{aux:openai-cluster-20}. 

After training, the amount of minimum computing power to perform DNN inference is generally a lot smaller, because we can load and compute neural layers one by one without storing intermediate activations. Nevertheless, inference may still require dozens of GPUs, a handful of memory and seconds to compute. Thus, a few research directions of Deep Learning are dedicated to reduce the required computing power, trying to preserve the similar model's quality.

One such approach is known as \textit{knowledge distillation}. Its idea, is that the learned values of DNN's parameters are not equivalent to knowledge of how to perform a task. In fact, by knowledge we can consider any sufficiently good mapping from input data to the desired output data \cite{method:distillation15}. Thus, we could try to train the original model fully to yield great results, and then to train another DNN "student" model with much fewer parameters. Both models are inferred with the same input data, and the student model is supervised to output the same results as the original model. Mimicking intermediate activations of the original model is also an option to replicate even more knowledge. The simplified neural architecture of the student models helps to reduce computation time and overfitting. That's because with fewer parameters there's less capacity in the student to remember the training data, and also imperfections of the original model's predictions additionally regularize the student \cite{survey:distillation21, speed:distillgan19}. However, knowledge distillation adds up more research work, with experimentation on neural architectures of the student, the training routines and hyper-parameters.

There's an adjacent group of methods, called \textit{neural architecture search}. It aims to automatically find a DNN architecture that would outperform hand-crafted architectures on the given tasks. The motivation is to both reduce the committal effect of selecting a sub-optimal neural architecture by researchers, and to also reduce the amount of parameters required to accomplish the same tasks \cite{survey:neural-arch-search19, dnn:eff-neural-arch-search18}. However, a single search step requires to train the current network, collect quality metrics and then update the architecture. The convergence may require dozens of steps. This limits neural architecture search to lightweight DNNs, and it's hard to apply for big neural architectures.

Another option comes from an observation, that some activation values of DNNs may be less important to the overall output than the others. Such activation values usually have low variance with respect to different input data, i.e. they frequently yield similar values. The corresponding learned parameters could then be replaced in the architecture by constants, if the quality doesn't decrease much. Such approach is called \textit{pruning}. Similarly to knowledge distillation, it allows to train a big model with the full capacity, which eases training. Then the obtained knowledge is squeezed to the lower number of parameters  \cite{method:pruning16, survey:pruning20, speed:prunning-gan21}.

On a computer hardware level, DNNs are usually trained and inferred with weights and activations represented with 16-bit/32-bit floating point numbers (also referred to as half-float or FP16; full-float or FP32 respectively). A \textit{quantization} approach switches to integer number formats, with lower memory usage, e.g. 4-bit/8-bit/16-bit integer numbers (INT4/INT8/INT16). The main motivation is that integer operations can be computed faster than floating-point ones, their physical circuits are smaller, thus decreasing device's size, overheating, energy consumption \cite{aux:fp-int-speed10}. The quantization approach relies on a fact, that a vector of floating-point numbers $\bm{x}$ can be approximately expressed as 
\begin{equation}
	\setlength\abovedisplayskip{0pt}
	\bm{x} \approx s^\prime_{\bm{x}} \cdot \bm{x^\prime_{\mathtt{int}}} = s_{\bm{x}} \cdot (\bm{x_{\mathtt{int}}} - z_{\bm{x}})
	\setlength\belowdisplayskip{0pt}
\end{equation} where $s^\prime_{\bm{x}}$ is a floating-point scalar, $\bm{x^\prime_{\mathtt{int}}}$ contains only integer values. Then they can be re-parameterized using an integer zero-offset $z_{\bm{x}}$, so that values in $\bm{x_{\mathtt{int}}}$ are mapped uniformly to range $(0, 2^b-1)$, with $b$ being bit-width of a number. Having a floating-point vector $\bm{x}$, it's quantized as:  
\begin{equation}
	\setlength\abovedisplayskip{0pt}
	\bm{x_{\mathtt{int}}} = \text{round}(\bm{x}/s_{\bm{x}}) + z_{\bm{x}}.
	\setlength\belowdisplayskip{0pt}
\end{equation}

Since many operations in neural networks can be computed using dot products of vectors, this allows to represent them in a quantized form. For example, a basic operation of matrix-vector multiplication with a floating-point weights matrix $\bm{W}_{n \times m}$ and a bias vector $\bm{b}_n$, the output activation vector $\bm{A}_n$  could be computed from an input activation vector $\bm{x}_m$ as:
\setlength\abovedisplayskip{0pt}
\begin{align}
\begin{split}
	\bm{A} &= \bm{b} + \bm{W} \bm{x}\\
  &\approx \bm{b}
  + [s_{\bm{w}} (\bm{W}_{\mathtt{int}} - z_{\bm{w}})]
  \cdot [s_{\bm{x}} (\bm{x}_{\mathtt{int}} - z_{\bm{x}})] \\
  &=\bm{b}
  + s_{\bm{w}} s_{\bm{x}} ( 
  \bm{W}_{\mathtt{int}} \bm{x}_{\mathtt{int}}
  - \bm{x}_{\mathtt{int}} z_{\bm{w}}
  \textcolor{blue}{- \bm{W}_{\mathtt{int}} z_{\bm{x}}
  + z_{\bm{x}} z_{\bm{w}}}).
	\setlength\belowdisplayskip{0pt}
\end{split}
\end{align} The last two terms shown in blue can be pre-computed for the whole DNN, and merged with the bias vector $\bm{b}$ \cite{dnn:quant-white21}. However since there's still an extra overhead from computation of $\bm{x}_{\mathtt{int}} z_{\bm{w}}$, specifically for weights it's common to use symmetric quantization, where floating-point values are mapped to $(-2^{b-1}, 2^{b-1}-1)$ integer range, with floating-point 0 mapped precisely to integer 0, thus making $z_{\bm{w}} = 0$, and eliminating the overhead. 

Typically, a post training quantization (PTQ) is applied to weights and activations of a DNN, it's easy to apply and automatically get performance benefits of quantization. On the other hand, if precision of the outputs is a priority, a  quantization-aware training (QAT) \cite{quant:qat18} approach is used. Here architecture tweaks are required to do quantized inference and weights updates, with an idea to make the model learn to deal with quantization imprecision. As a downside, it enlarges training time and adds issues with analytic gradients back-propagation through rounded values \cite{quant:straight-through-estimator13}.

Usually, the uniform quantization is appealing for simplicity of hardware implementation. However, the quantization range defined by constants $s$ and $z$ may be selected too wide, e.g. min-max range of values in a tensor where outliers are present. Thus, a lot of precision will be lost as different numbers will be mapped to the same integer value. Instead, the range can be selected to minimize a mean squared error with respect to the original floating-point values \cite{quant:mse19, speed:quant-error-analysis15}. If hardware supports it, instead of defining per-tensor quantization constants, they can be defined per-channel for more precision. Alternatively, a Cross Layer Equalization \cite{quant:cle20} can be applied to re-parameterize quantized weights and biases, compensating the channels' discrepancy of ranges, without adding runtime overhead. A non-uniform quantization can also be used to devote more precision to near mean values, and less to the outliers, but the hardware support of it is very limited \cite{quant:non-uniform21}.

Moving on, DNNs also suffer from computational inefficiency, when similar input data leads to computing identical activations of layers. The DNN's architecture can be designed to cache and reuse these values. It can be applied to video stream processing or synthesis, where multiple consecutive frames have similarity \cite{aux:reusing19}. Additionally, filters of CNNs may frequently contain repeated weights after being quantized. Thus, as a filter slides over the data, certain dot-products can be reused \cite{dnn:reusing18}. While having a potential to speed up a certain DNN architecture, it requires immense manual hand-craft, which slows down active research and experimentation.

The last but not least appoach, is to design the neural architectures efficiently, i.e with a good trade off of inference speed and outputs accuracy. It's obtained by reducing the number of parameters, applying only fast-to-compute neural layers, and composing them into a sparse computational graph. It leads to a better utilization of memory caches and computing cores of the hardware. Often, such models are implemented using low-level parallelized code, that increases the number of floating-point operations per second (FLOPs) even further. The notable examples are: MobileNetV1 \cite{dnn:mnv1-17}, MobileNetV2 \cite{dnn:mnv2-18}, MobileNetV3 \cite{dnn:mnv3-19},  ShuffleNetV1 \cite{dnn:shufflenetv1-18}, ShuffleNetV2 \cite{dnn:shufflenetv2-18}, CondenseNetV1 \cite{dnn:condensenetv1-18}, CondenseNetV2 \cite{dnn:condensenetv2-21}, EfficientNet \cite{dnn:efficientnetv1-19}. Although the original papers obtain similar or better results with their proposed architectures, the drop-in replacement is not guaranteed for every task. In this work a significant drop of quality is observed, when a baseline ResNet18 backbone was replaced with either MobileNetV2 or EfficientNet.

%

\section{Architecture of mobile devices}
\label{lit:mobile}

For a long time now, the computing hardware has been moving from general purpose computing units (CPU), towards specialized processing units, that are specifically constructed and programmed to perform a subset of operations faster than CPU. The most obvious example is a Graphical Processing Unit (GPU), consisting of hundreds of repetitive cores that may execute linear algebra operations in parallel. The usage of GPUs was recognized for AI purposes. Powerful full-size GPUs are the main devices to train and execute DNNs on desktop computers and clusters. On embedded hardware (for instance mobile), the technology goes further by introducing the more specialized accelerated devices, that complete certain tasks faster than mobile GPUs \cite{mobile:dl-review19}.

Different accelerated devices exist under separate names, but the similar idea - specialized computing. For example, mobile SoC by Samsung contains Neural Processing Units (NPU) for tensor computations. Google has dedicated Tensor Processing Units (TPU) for large-scale tensor computations on base of clusters of computer machines.

Qualcomm Snapdragon SoC, that is of interest for this project, may contain a Digital Signals Processor (DSP) accelerated device (besides CPU and GPU). It's a chip designed to perform low power quantized computations with built-in support to parallel execution and Single Instruction Multiple Data operations \cite{speed:online-dsp-qualcomm}.

Mobile hardware has a lot of minor differences from desktop computing hardware. One of those is that the GPU is physically located on the same chip as the CPU. Also, their memory is located on the same chip, yet mutually inaccessible in software.

Although technically hard, but accelerated devices can be programmed separately. \cite{mobile:pipelining20} suggests a pipeline approach, where a model is split into several stages. Instead of processing input from start to finish, each stage receives new input as soon as it finished the previous calculation. By combining usage of multiple accelerate devices on mobile hardware, it's possible to almost double real time performance.

 Data-free quantization \cite{speed:datafreequant19} algorithm is proposed and implemented for Qualcomm Snapdragon SoCs, and achieves near-identical outputs' quality for quantized DNNs
