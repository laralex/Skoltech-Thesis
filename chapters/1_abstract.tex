\emergencystretch 3em

\par
During development of artificial intelligence models, the utmost attention is brought to quality (accuracy) of their outputs. However, some models are meant to be executed on limited user hardware. Thus, computational resources also require careful consideration, when it's necessary to sustain high performance.

\par
This master thesis describes implementation of an application with an integrated Deep Neural Network, for mobile devices on Qualcomm Snapdragon platform. The network generates images of full-body 3D human avatars, in real-time, more than 30 times per second. The generation takes into account position and orientation of the mobile device. When avatar images are visualized on top of camera frames, the Augmented Reality (AR) effect emerges. A user can observe the avatar from different angles, either from afar or close up. Thus, the generated images should keep the visual quality from angles, unseen in the training data. The first contribution of this thesis is organization of a computational pipeline to achieve real-time performance. Secondly, the ways of increasing the visual quality were researched, that would be robust to arbitrary view points.

\par
As a result, the baseline model \cite{dnn:stylepeople21} can be executed in the mobile application in real time, up to $640\times640$ pixels resolution. In order to increase visual quality in the AR application, the neural network receives and outputs images, such that the observed portion of the body occupies as much frame space as possible. The approach of neural network training was extended, to learn and produce images in either full-body scale or close-up scale. The faced visual artifacts, that originate from modality difference of those scales, were solved. 

\par
\textbf{Keywords:} Real-Time Performance, Mobile Hardware, Neural Rendering, Deep Neural Network, Human Avatar Reconstruction, Computer Graphics.  
