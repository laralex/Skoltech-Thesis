\emergencystretch 3em

\par
During the development of artificial intelligence models, the utmost attention is brought to the quality (accuracy) of their outputs. However, some models are meant to be executed on limited user hardware. Thus, the computational resources also require careful consideration, when it is necessary to sustain high performance.

\par
This thesis describes a mobile application implementation for devices on the Qualcomm Snapdragon hardware platform. The application integrates a Deep Neural Network, that generates images of full-body 3D human avatars. The generation takes into account the position and orientation of the mobile device. When avatar images are visualized on top of camera frames, the Augmented Reality (AR) effect emerges. A user can observe the avatar from different viewpoints, either from afar or close up. Thus, the generated images should keep the visual quality for angles, that never appeared in the training data. The first contribution of this thesis is the organization of a computational pipeline to achieve the real-time performance. Secondly, we research ways of increasing the visual quality, that would be robust to arbitrary viewpoints.

\par
As a result, we execute the baseline model \cite{dnn:stylepeople21} in the mobile application in real-time, with more than 30 frames per second and up to $640\times640$ pixels resolution. We generate the model's input, such that the observed portion of the body occupies as much of the frame space as possible, thus increasing  the visual quality in the AR application. We extend the training procedure of the neural network training, to learn and produce images in either full-body scale or close-up scale. We propose solutions to partially prevent the visual artifacts that appear with the baseline training approach. 

\par
\textbf{Keywords:} Real-Time Performance, Mobile Hardware, Neural Rendering, Deep Neural Network, Human Avatar Reconstruction, Computer Graphics.  
