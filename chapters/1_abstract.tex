\emergencystretch 3em

\par
During development of artificial intelligence models, the utmost attention is brought to quality (accuracy) of their outputs. However, some models are meant to be executed on limited user hardware. Thus, computational resources also require careful consideration, when it is necessary to sustain high performance.

\par
This master thesis describes implementation of an application with an integrated Deep Neural Network, for mobile devices on Qualcomm Snapdragon platform. The neural network generates images of full-body 3D human avatars. The generation takes into account position and orientation of the mobile device. When avatar images are visualized on top of camera frames, the Augmented Reality (AR) effect emerges. A user can observe the avatar from different angles, either from afar or close up. Thus, the generated images should keep the visual quality from angles, unseen in the training data. The first contribution of this thesis is organization of a computational pipeline to achieve real-time performance. Secondly, we carry out research on ways of increasing the visual quality, that would be robust to arbitrary view points.

\par
As a result, we execute the baseline model \cite{dnn:stylepeople21} in the mobile application in real time, with more than 30 frames per second and up to $640\times640$ pixels resolution. We generate the model's input, such that the observed portion of the body occupies as much frame space as possible, thus increasing visual quality in the AR application. We extend the training procedure of the neural network training, to learn and produce images in either full-body scale or close-up scale. We propose solutions to partially prevent visual artifacts that appear with the baseline training pipeline. 

\par
\textbf{Keywords:} Real-Time Performance, Mobile Hardware, Neural Rendering, Deep Neural Network, Human Avatar Reconstruction, Computer Graphics.  
